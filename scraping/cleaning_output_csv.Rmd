---
title: "cleaning_outputcsv"
author: "Ingrid Backman"
date: "2024-04-16"
output: html_document
---
```{r}
test <- read.csv("combined_run2.1.csv")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r 1st run}
#cleaning scraped data royal society to make sure theres no duplicates
data <- read.csv("scraped_royalsociety_for_now.csv")
original_rows <- nrow(data)
data_unique <- unique(data)
unique_rows <- nrow(data_unique)
duplicate_rows <- data[!duplicated(data), ]

rows_removed <- data[!rownames(data) %in% rownames(data_unique), ]

write.csv(data_unique, "output_unique.csv", row.names = FALSE)

cat("Number of rows before removing duplicates:", original_rows, "\n")
cat("Number of rows after removing duplicates:", unique_rows, "\n")

print(rows_removed)

```

```{r 2nd run}
#combining and cleaning data together from the 2nd run (rstb domain of royal society)
library(dplyr)

combined_run_now <- read.csv("combined_run2.csv")
file_list <- list.files(pattern = "royalsociety2ndrun.*\\.csv$|royalsociety3.*\\.csv$")

new_combined_df <- lapply(file_list, read.csv) %>%
  bind_rows() %>%
  distinct(og_url, .keep_all = TRUE) %>%
  filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))

new_combined_df <- new_combined_df[, colSums(is.na(new_combined_df)) != nrow(new_combined_df)]

final_combined_df <- bind_rows(combined_run_now, new_combined_df) %>%
  distinct(identifier, .keep_all = TRUE)

final_combined_df <- final_combined_df[, colSums(is.na(final_combined_df)) != nrow(final_combined_df)]

head(final_combined_df)

write.csv(final_combined_df, "combined_run2.1.csv", row.names = FALSE)

```


```{r 2nd run}
write.csv(combined_df, "combined_run2.csv", row.names = FALSE)

```


```{r 2nd run}
library(dplyr)
visited_urls_df <- read.csv("visited_urls.csv")

repeat_count <- visited_urls_df %>%
  group_by_all() %>% 
  filter(n() > 1) %>% 
  nrow() 

print(paste("Number of repeated rows:", repeat_count))
#thats way too many repeated rows...
```


```{r 2nd run}
library(dplyr)
restricted_urls_df <- read.csv("restricted_urls.csv")

restricted_urls_df <- restricted_urls_df %>%
  group_by_all() %>% 
  unique()

write.csv(restricted_urls_df, "restricted_urls.csv", row.names = FALSE)
```


```{r 2nd run}
#making a new visited urls file
library(dplyr)

final_combined_df <- read.csv("combined_run2.1.csv")
restricted_urls_df <- read.csv("restricted_urls.csv", header = FALSE, col.names = "og_url")

visited_urls_new_df <- final_combined_df %>%
  select(og_url) %>%
  distinct()

restricted_urls_new_df <- restricted_urls_df %>%
  select(og_url) %>%
  distinct()

all_visited_urls_df <- bind_rows(visited_urls_new_df, restricted_urls_new_df) %>%
  distinct()

write.csv(all_visited_urls_df, "visited_urls.csv", row.names = FALSE)

head(all_visited_urls_df)


```



```{r spectator1}
#quick ChatGPT nonsense to clean and split the spectator text file at https://www.gutenberg.org/files/12030/12030-h/12030-h.htm

# Read the content of the spectator.txt file
spectator <- readLines("prog/spectator.txt")

# Convert the content to a single string
spectator_text <- paste(spectator, collapse = "\n")

# Define the regex pattern to match "No. " followed by one to three digits
# and ensure a weekday occurs within the next 100 characters, or "Preface\n"
pattern <- "(No\\.\\s\\d{1,3}\\s(?=[\\s\\S]{0,100}(Thursday|Monday|Tuesday|Wednesday|Friday|Saturday|Sunday)))|Preface\\n"

# Use gregexpr to find match positions
matches <- gregexpr(pattern, spectator_text, perl = TRUE)
match_positions <- unlist(matches)

# Initialize a list to store the split text chunks
spectator_split <- list()

# Iterate over match positions to extract chunks
for (i in seq_along(match_positions)) {
  start_pos <- match_positions[i]
  if (i < length(match_positions)) {
    end_pos <- match_positions[i + 1] - 1
  } else {
    end_pos <- nchar(spectator_text)
  }
  
  chunk <- substring(spectator_text, start_pos, end_pos)
  
  # Only include the chunk if it is at least 200 characters long
  if (nchar(chunk) >= 200) {
    spectator_split <- c(spectator_split, list(chunk))
  }
}

# Initialize vectors to store section numbers and content
section_numbers <- character()
section_content <- character()

# Iterate over the chunks and extract section numbers and content
for (chunk in spectator_split) {
  # Extract section number
  section_num <- ifelse(grepl("Preface\\n", chunk), "Preface", 
                        gsub("No\\. (\\d+).*", "\\1", grep("No\\. (\\d+)", chunk, value = TRUE)))
  
  # Append to section numbers vector
  section_numbers <- c(section_numbers, section_num)
  
  # Append to section content vector
  section_content <- c(section_content, chunk)
}

# Create dataframe
spectator_df <- data.frame(Section_Number = section_numbers, Content = section_content)

```


```{r spectator2}
# possibly for cleaning up the text but not really necessary
# spectator_df$Content <- gsub("\n", " ", spectator_df$Content)
# spectator_df$Content <- gsub("\t", " ", spectator_df$Content)
# spectator_df$Content <- gsub(" {1,}", " ", spectator_df$Content)


# Iterate over each row of the dataframe
for (i in seq_len(nrow(spectator_df))) {
  # Extract section number and content for the current row
  section_number <- spectator_df$Section_Number[i]
  content <- spectator_df$Content[i]
  
  # Generate file name
  file_name <- paste0("prog/", section_number, ".txt")
  
  # Write content to file
  writeLines(content, file_name)
}


```

