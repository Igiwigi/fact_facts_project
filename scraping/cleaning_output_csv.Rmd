---
title: "cleaning_outputcsv"
author: "Ingrid Backman"
date: "2024-04-16"
output: html_document
---
```{r}
test <- read.csv("combined_run2.1.csv")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r 1st run}
#cleaning scraped data royal society to make sure theres no duplicates
data <- read.csv("data/royalsociety2ndrun2.csv")
original_rows <- nrow(data)
data_unique <- unique(data)
unique_rows <- nrow(data_unique)
duplicate_rows <- data[!duplicated(data), ]

rows_removed <- data[!rownames(data) %in% rownames(data_unique), ]

write.csv(data_unique, "2nd_run.csv", row.names = FALSE)

cat("Number of rows before removing duplicates:", original_rows, "\n")
cat("Number of rows after removing duplicates:", unique_rows, "\n")

print(rows_removed)

```

```{r 3rd run}
#combining and cleaning data together from the 3rd run (rsta domain of royal society)
library(dplyr)

file_list <- list.files(pattern = "royalsociety_rsta.*\\.csv")

new_combined_df <- lapply(file_list, read.csv) %>%
  bind_rows() %>%
  distinct(og_url, .keep_all = TRUE) %>%
  filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))


final_combined_df <- bind_rows(new_combined_df) %>%
  distinct(identifier, .keep_all = TRUE)

final_combined_df <- final_combined_df[, colSums(is.na(final_combined_df)) != nrow(final_combined_df)]

head(final_combined_df)

write.csv(final_combined_df, "3rd_run_combined.csv", row.names = FALSE)

```



```{r 3rd run}
#making a new visited urls file
library(dplyr)

final_combined_df <- read.csv("3rd_run_combined.csv")
restricted_urls_df <- read.csv("restricted_urls.csv", header = FALSE, col.names = "og_url")

visited_urls_new_df <- final_combined_df %>%
  select(og_url) %>%
  distinct()

restricted_urls_new_df <- restricted_urls_df %>%
  select(og_url) %>%
  distinct()

all_visited_urls_df <- bind_rows(visited_urls_new_df, restricted_urls_new_df) %>%
  distinct()

write.csv(all_visited_urls_df, "visited_urls.csv", row.names = FALSE)

head(all_visited_urls_df)


```



```{r spectator1}
#quick ChatGPT nonsense to clean and split the spectator text file at https://www.gutenberg.org/files/12030/12030-h/12030-h.htm

# Read the content of the spectator.txt file
spectator <- readLines("prog/spectator.txt")

# Convert the content to a single string
spectator_text <- paste(spectator, collapse = "\n")

# Define the regex pattern to match "No. " followed by one to three digits
# and ensure a weekday occurs within the next 100 characters, or "Preface\n"
pattern <- "(No\\.\\s\\d{1,3}\\s(?=[\\s\\S]{0,100}(Thursday|Monday|Tuesday|Wednesday|Friday|Saturday|Sunday)))|Preface\\n"

# Use gregexpr to find match positions
matches <- gregexpr(pattern, spectator_text, perl = TRUE)
match_positions <- unlist(matches)

# Initialize a list to store the split text chunks
spectator_split <- list()

# Iterate over match positions to extract chunks
for (i in seq_along(match_positions)) {
  start_pos <- match_positions[i]
  if (i < length(match_positions)) {
    end_pos <- match_positions[i + 1] - 1
  } else {
    end_pos <- nchar(spectator_text)
  }
  
  chunk <- substring(spectator_text, start_pos, end_pos)
  
  # Only include the chunk if it is at least 200 characters long
  if (nchar(chunk) >= 200) {
    spectator_split <- c(spectator_split, list(chunk))
  }
}

# Initialize vectors to store section numbers and content
section_numbers <- character()
section_content <- character()

# Iterate over the chunks and extract section numbers and content
for (chunk in spectator_split) {
  # Extract section number
  section_num <- ifelse(grepl("Preface\\n", chunk), "Preface", 
                        gsub("No\\. (\\d+).*", "\\1", grep("No\\. (\\d+)", chunk, value = TRUE)))
  
  # Append to section numbers vector
  section_numbers <- c(section_numbers, section_num)
  
  # Append to section content vector
  section_content <- c(section_content, chunk)
}

# Create dataframe
spectator_df <- data.frame(Section_Number = section_numbers, Content = section_content)

```


```{r spectator2}
# possibly for cleaning up the text but not really necessary
# spectator_df$Content <- gsub("\n", " ", spectator_df$Content)
# spectator_df$Content <- gsub("\t", " ", spectator_df$Content)
# spectator_df$Content <- gsub(" {1,}", " ", spectator_df$Content)


# Iterate over each row of the dataframe
for (i in seq_len(nrow(spectator_df))) {
  # Extract section number and content for the current row
  section_number <- spectator_df$Section_Number[i]
  content <- spectator_df$Content[i]
  
  # Generate file name
  file_name <- paste0("prog/", section_number, ".txt")
  
  # Write content to file
  writeLines(content, file_name)
}


```

```{r}
#check if any of the rstb pages (2nd_run_combined.csv) are already in the first run in old/1st_run.csv. omit from 2nd_run_combined.csv the ones that are shared by both
# Load necessary library
# Load necessary library
library(dplyr)

first_run <- read.csv("old/1st_run.csv", stringsAsFactors = FALSE)
second_run <- read.csv("2nd_run_combined.csv", stringsAsFactors = FALSE)

filtered_second_run <- second_run %>%
  filter(!(identifier %in% first_run$identifier))

rows_filtered_out <- nrow(second_run) - nrow(filtered_second_run)

print(paste(rows_filtered_out, "rows were filtered out."))

write.csv(filtered_second_run, "2nd_run_filtered.csv", row.names = FALSE)

print("Filtering completed. Saved as 2nd_run_filtered.csv.")





```

```{r}
#fixing doi names for later download
second_run <- read.csv("2nd_run_combined.csv")

# Assuming your data frame is called 'df'
second_run$identifier <- ifelse(
  grepl("^rstb", second_run$identifier), # Check if the identifier starts with 'rstb'
  gsub("https://royalsocietypublishing.org/doi/", "", second_run$og_url), # Extract the DOI part from 'og_url'
  second_run$identifier # Keep the original identifier if it's already in the correct format
)

# Preview the fixed column
print(second_run$identifier)

write.csv(second_run, "2nd_run_combined.csv", row.names = FALSE)


```


```{r creating_metadata}
data <- read.csv("2nd_run_combined.csv") #rstb

#the metadata file for royal society (i dont remember how i originally did it)
data <- data %>%
  mutate(downloaded = TRUE) %>%
  mutate(filename = stri_replace_all_regex(
    identifier,
    pattern = c('10.1098/', "\\."),
    replacement = c("", "_"),
    vectorize = FALSE
  )) %>%
  mutate(filename = stri_replace_all_regex(
    filename,
    pattern = c(' ', ',', ';', "\\.", "__", "'"),
    replacement = c("_", "", "", "", "_", ""),
    vectorize = FALSE
  )) %>%
  mutate(filename = paste0(filename, ".pdf"))

write.csv(data, "royalsociety_metadata2.csv", row.names = FALSE)
```

```{r combining_metadata}
old_metadata <- read.csv("D:/Fact_fiction_corpus/texts/royal society/royalsociety_metadata.csv")

new_metadata <- read.csv("royalsociety_metadata2.csv")

#combine them into one, remove duplicates, write it as royalsociety_metadata_new.csv
combined_metadata <- bind_rows(old_metadata, new_metadata) %>%
  distinct(identifier, .keep_all = TRUE)

write.csv(combined_metadata, "royalsociety_metadata_new.csv", row.names = FALSE)

#load the new metadata file but only the filename column
metadata <- read.csv("royalsociety_metadata_new.csv") %>%
  select(filename)

```

