knitr::opts_chunk$set(echo = TRUE)
# Read the CSV file
data <- read.csv("outputfile.csv")
# Number of rows before removing duplicates
original_rows <- nrow(data)
# Remove duplicate rows
data_unique <- unique(data)
# Number of rows after removing duplicates
unique_rows <- nrow(data_unique)
# Identify duplicate rows
duplicate_rows <- data[!duplicated(data), ]
# Rows that are being removed
rows_removed <- data[!rownames(data) %in% rownames(data_unique), ]
# Write the unique data to a new CSV file
write.csv(data_unique, "output_unique.csv", row.names = FALSE)
# Print the number of rows before and after removing duplicates
cat("Number of rows before removing duplicates:", original_rows, "\n")
cat("Number of rows after removing duplicates:", unique_rows, "\n")
# Print the rows being removed
print(rows_removed)
duplicate_rows
# Read the CSV file
data <- read.csv("output_unique.csv")
# Number of rows before removing duplicates
original_rows <- nrow(data)
# Remove duplicate rows
data_unique <- unique(data)
# Number of rows after removing duplicates
unique_rows <- nrow(data_unique)
# Identify duplicate rows
duplicate_rows <- data[!duplicated(data), ]
# Rows that are being removed
rows_removed <- data[!rownames(data) %in% rownames(data_unique), ]
# Write the unique data to a new CSV file
write.csv(data_unique, "output_unique2.csv", row.names = FALSE)
# Print the number of rows before and after removing duplicates
cat("Number of rows before removing duplicates:", original_rows, "\n")
cat("Number of rows after removing duplicates:", unique_rows, "\n")
# Print the rows being removed
print(rows_removed)
knitr::opts_chunk$set(echo = TRUE)
# Read the CSV file
data <- read.csv("scraped_royalsociety_for_now.csv")
# Number of rows before removing duplicates
original_rows <- nrow(data)
# Remove duplicate rows
data_unique <- unique(data)
# Number of rows after removing duplicates
unique_rows <- nrow(data_unique)
# Identify duplicate rows
duplicate_rows <- data[!duplicated(data), ]
# Rows that are being removed
rows_removed <- data[!rownames(data) %in% rownames(data_unique), ]
# Write the unique data to a new CSV file
write.csv(data_unique, "output_unique.csv", row.names = FALSE)
# Print the number of rows before and after removing duplicates
cat("Number of rows before removing duplicates:", original_rows, "\n")
cat("Number of rows after removing duplicates:", unique_rows, "\n")
# Print the rows being removed
print(rows_removed)
original_rows
View(data)
View(data)
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("scraped_royalsociety_for_now.csv")
data <- read.csv("scraped_royalsociety_for_now.csv")
View(data)
View(data)
View(data)
?download.file
data <- read.csv("scraped_royalsociety_for_now.csv")
#iterate through data and download each file from the download link under the column pdf_download_link with a timer of 1 second between downloads, put them into the destination folder (the downloaded file) under the same name as the title column of that row
for (i in 1:nrow(data)) {
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb")
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#iterate through data and download each file from the download link under the column pdf_download_link with a timer of 1 second between downloads, put them into the destination folder (the downloaded file) under the same name as the title column of that row
#do something to bypass the http 403 status
for (i in 1:nrow(data)) {
print(data$pdf_download_link[i])
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb")
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#iterate through data and download each file from the download link under the column pdf_download_link with a timer of 1 second between downloads, put them into the destination folder (the downloaded file) under the same name as the title column of that row
#do something to bypass the http 403 status
for (i in 1:nrow(data)) {
print(data$pdf_download_link[i])
print(data$title[i])
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb")
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#iterate through data and download each file from the download link under the column pdf_download_link with a timer of 1 second between downloads, put them into the destination folder (the downloaded file) under the same name as the title column of that row
#maybe add more to the download name so it can also encompass date information as well as doi
for (i in 1:nrow(data)) {
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb", headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'))
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
?download.file
data <- read.csv("scraped_royalsociety_for_now.csv")
#add a user agent to the download.file function to avoid being blocked by the website
for (i in 1:nrow(data)) {
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb", headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'), headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36')
print(data$title[i] + " has been downloaded, maybe")
data <- read.csv("scraped_royalsociety_for_now.csv")
#add a user agent to the download.file function to avoid being blocked by the website
for (i in 1:nrow(data)) {
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb", headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'), headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'))
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#add a user agent to the download.file function to avoid being blocked by the website
for (i in 1:nrow(data)) {
print(base::curlGetHeaders(data$pdf_download_link[i]))
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#add a user agent to the download.file function to avoid being blocked by the website
for (i in 1:nrow(data)) {
print(base::curlGetHeaders(data$pdf_download_link[1]))
}
knitr::opts_chunk$set(echo = TRUE)
visited_urls_df <- read.csv("visited_urls.csv")
repeat_count <- visited_urls_df %>%
group_by_all() %>%
filter(n() > 1) %>%
nrow()
library(dplyr)
visited_urls_df <- read.csv("visited_urls.csv")
repeat_count <- visited_urls_df %>%
group_by_all() %>%
filter(n() > 1) %>%
nrow()
print(paste("Number of repeated rows:", repeat_count))
#thats way too many repeated rows...
View(visited_urls_df)
library(dplyr)
visited_urls_df <- read.csv("restricted_urls.csv")
repeat_count <- visited_urls_df %>%
group_by_all() %>%
filter(n() > 1) %>%
nrow()
print(paste("Number of repeated rows:", repeat_count))
#thats way too many repeated rows...
visited_urls_df <- read.csv("restricted_urls.csv")
library(dplyr)
visited_urls_df <- read.csv("restricted_urls.csv")
visited_urls_df <- visited_urls_df %>%
group_by_all() %>%
filter(n() > 1) %>%
nrow()
print(paste("Number of repeated rows:", repeat_count))
#thats way too many repeated rows...
library(dplyr)
visited_urls_df <- read.csv("restricted_urls.csv")
visited_urls_df <- visited_urls_df %>%
group_by_all() %>%
filter(n() > 1)
print(paste("Number of repeated rows:", repeat_count))
#thats way too many repeated rows...
visited_urls_df <- read.csv("restricted_urls.csv")
library(dplyr)
visited_urls_df <- read.csv("restricted_urls.csv")
visited_urls_df <- visited_urls_df %>%
group_by_all() %>%
filter(n() = 1)
library(dplyr)
visited_urls_df <- read.csv("restricted_urls.csv")
visited_urls_df <- visited_urls_df %>%
group_by_all() %>%
unique()
print(paste("Number of repeated rows:", repeat_count))
#thats way too many repeated rows...
library(dplyr)
restricted_urls_df <- read.csv("restricted_urls.csv")
restricted_urls_df <- restricted_urls_df %>%
group_by_all() %>%
unique()
print(paste("Number of repeated rows:", restricted_urls_df))
#thats way too many repeated rows...
restricted_urls_df <- read.csv("restricted_urls.csv")
library(dplyr)
restricted_urls_df <- read.csv("restricted_urls.csv")
restricted_urls_df <- restricted_urls_df %>%
group_by_all() %>%
unique()
write.csv(restricted_urls_df, "restricted_urls.csv", row.names = FALSE)
library(dplyr)
restricted_urls_df <- read.csv("restricted_urls.csv")
restricted_urls_df <- restricted_urls_df %>%
group_by_all() %>%
unique()
write.csv(restricted_urls_df, "restricted_urls.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
#combining and cleaning data together from the 3rd run (rstb domain of royal society)
library(dplyr)
#combined_run_now <- read.csv("royalsociety_rsta_latest2.csv")
file_list <- list.files(pattern = "royalsociety_rsta.*\\.csv")
new_combined_df <- lapply(file_list, read.csv) %>%
bind_rows() %>%
distinct(og_url, .keep_all = TRUE) %>%
filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))
new_combined_df <- new_combined_df[, colSums(is.na(new_combined_df)) != nrow(new_combined_df)]
final_combined_df <- bind_rows(combined_run_now, new_combined_df) %>%
distinct(identifier, .keep_all = TRUE)
#combining and cleaning data together from the 3rd run (rstb domain of royal society)
library(dplyr)
combined_run_now <- read.csv("royalsociety_rsta_latest2.csv")
file_list <- list.files(pattern = "royalsociety_rsta.*\\.csv")
new_combined_df <- lapply(file_list, read.csv) %>%
bind_rows() %>%
distinct(og_url, .keep_all = TRUE) %>%
filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))
new_combined_df <- new_combined_df[, colSums(is.na(new_combined_df)) != nrow(new_combined_df)]
final_combined_df <- bind_rows(combined_run_now, new_combined_df) %>%
distinct(identifier, .keep_all = TRUE)
final_combined_df <- final_combined_df[, colSums(is.na(final_combined_df)) != nrow(final_combined_df)]
head(final_combined_df)
write.csv(final_combined_df, "3rd_run_combined.csv", row.names = FALSE)
View(final_combined_df)
#combining and cleaning data together from the 3rd run (rstb domain of royal society)
library(dplyr)
file_list <- list.files(pattern = "royalsociety_rsta.*\\.csv")
new_combined_df <- lapply(file_list, read.csv) %>%
bind_rows() %>%
distinct(og_url, .keep_all = TRUE) %>%
filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))
final_combined_df <- bind_rows(new_combined_df) %>%
distinct(identifier, .keep_all = TRUE)
final_combined_df <- final_combined_df[, colSums(is.na(final_combined_df)) != nrow(final_combined_df)]
head(final_combined_df)
write.csv(final_combined_df, "3rd_run_combined.csv", row.names = FALSE)
View(new_combined_df)
View(final_combined_df)
View(final_combined_df)
visited_urls_df <- read.csv("visited_urls.csv")
repeat_count <- visited_urls_df %>%
group_by_all() %>%
filter(n() > 1) %>%
nrow()
print(paste("Number of repeated rows:", repeat_count))
#thats way too many repeated rows...
#making a new visited urls file
library(dplyr)
final_combined_df <- read.csv("3rd_run_combined.csv")
restricted_urls_df <- read.csv("restricted_urls.csv", header = FALSE, col.names = "og_url")
visited_urls_new_df <- final_combined_df %>%
select(og_url) %>%
distinct()
restricted_urls_new_df <- restricted_urls_df %>%
select(og_url) %>%
distinct()
all_visited_urls_df <- bind_rows(visited_urls_new_df, restricted_urls_new_df) %>%
distinct()
write.csv(all_visited_urls_df, "visited_urls.csv", row.names = FALSE)
head(all_visited_urls_df)
View(all_visited_urls_df)
visited_urls_df <- read.csv("restricted_urls.csv")
repeat_count <- visited_urls_df %>%
group_by_all() %>%
filter(n() > 1) %>%
nrow()
print(paste("Number of repeated rows:", repeat_count))
#thats way too many repeated rows...
#combining and cleaning data together from the 3rd run (rsta domain of royal society)
library(dplyr)
file_list <- list.files(pattern = "old/3rd_run/royalsociety_rsta.*\\.csv")
new_combined_df <- lapply(file_list, read.csv) %>%
bind_rows() %>%
distinct(og_url, .keep_all = TRUE) %>%
filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))
#combining and cleaning data together from the 3rd run (rsta domain of royal society)
library(dplyr)
file_list <- list.files(pattern = "old/3rd_run/royalsociety_rsta.*\\.csv")
new_combined_df <- lapply(file_list, read.csv) %>%
bind_rows() %>%
distinct(og_url, .keep_all = TRUE) %>%
filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))
#combining and cleaning data together from the 3rd run (rsta domain of royal society)
library(dplyr)
file_list <- list.files(pattern = "royalsociety_rsta.*\\.csv")
new_combined_df <- lapply(file_list, read.csv) %>%
bind_rows() %>%
distinct(og_url, .keep_all = TRUE) %>%
filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))
#combining and cleaning data together from the 3rd run (rsta domain of royal society)
library(dplyr)
file_list <- list.files(pattern = "royalsociety_rsta.*\\.csv")
new_combined_df <- lapply(file_list, read.csv) %>%
bind_rows() %>%
distinct(og_url, .keep_all = TRUE) %>%
filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))
#combining and cleaning data together from the 3rd run (rsta domain of royal society)
library(dplyr)
file_list <- list.files(pattern = "royalsociety_rsta.*\\.csv")
new_combined_df <- lapply(file_list, read.csv) %>%
bind_rows() %>%
distinct(og_url, .keep_all = TRUE) %>%
filter(!(author == "author" | rowSums(is.na(.)) == ncol(.)))
final_combined_df <- bind_rows(new_combined_df) %>%
distinct(identifier, .keep_all = TRUE)
final_combined_df <- final_combined_df[, colSums(is.na(final_combined_df)) != nrow(final_combined_df)]
head(final_combined_df)
write.csv(final_combined_df, "3rd_run_combined.csv", row.names = FALSE)
#making a new visited urls file
library(dplyr)
final_combined_df <- read.csv("3rd_run_combined.csv")
restricted_urls_df <- read.csv("restricted_urls.csv", header = FALSE, col.names = "og_url")
visited_urls_new_df <- final_combined_df %>%
select(og_url) %>%
distinct()
restricted_urls_new_df <- restricted_urls_df %>%
select(og_url) %>%
distinct()
all_visited_urls_df <- bind_rows(visited_urls_new_df, restricted_urls_new_df) %>%
distinct()
write.csv(all_visited_urls_df, "visited_urls.csv", row.names = FALSE)
head(all_visited_urls_df)
