knitr::opts_chunk$set(echo = TRUE)
# Read the CSV file
data <- read.csv("outputfile.csv")
# Number of rows before removing duplicates
original_rows <- nrow(data)
# Remove duplicate rows
data_unique <- unique(data)
# Number of rows after removing duplicates
unique_rows <- nrow(data_unique)
# Identify duplicate rows
duplicate_rows <- data[!duplicated(data), ]
# Rows that are being removed
rows_removed <- data[!rownames(data) %in% rownames(data_unique), ]
# Write the unique data to a new CSV file
write.csv(data_unique, "output_unique.csv", row.names = FALSE)
# Print the number of rows before and after removing duplicates
cat("Number of rows before removing duplicates:", original_rows, "\n")
cat("Number of rows after removing duplicates:", unique_rows, "\n")
# Print the rows being removed
print(rows_removed)
duplicate_rows
# Read the CSV file
data <- read.csv("output_unique.csv")
# Number of rows before removing duplicates
original_rows <- nrow(data)
# Remove duplicate rows
data_unique <- unique(data)
# Number of rows after removing duplicates
unique_rows <- nrow(data_unique)
# Identify duplicate rows
duplicate_rows <- data[!duplicated(data), ]
# Rows that are being removed
rows_removed <- data[!rownames(data) %in% rownames(data_unique), ]
# Write the unique data to a new CSV file
write.csv(data_unique, "output_unique2.csv", row.names = FALSE)
# Print the number of rows before and after removing duplicates
cat("Number of rows before removing duplicates:", original_rows, "\n")
cat("Number of rows after removing duplicates:", unique_rows, "\n")
# Print the rows being removed
print(rows_removed)
knitr::opts_chunk$set(echo = TRUE)
# Read the CSV file
data <- read.csv("scraped_royalsociety_for_now.csv")
# Number of rows before removing duplicates
original_rows <- nrow(data)
# Remove duplicate rows
data_unique <- unique(data)
# Number of rows after removing duplicates
unique_rows <- nrow(data_unique)
# Identify duplicate rows
duplicate_rows <- data[!duplicated(data), ]
# Rows that are being removed
rows_removed <- data[!rownames(data) %in% rownames(data_unique), ]
# Write the unique data to a new CSV file
write.csv(data_unique, "output_unique.csv", row.names = FALSE)
# Print the number of rows before and after removing duplicates
cat("Number of rows before removing duplicates:", original_rows, "\n")
cat("Number of rows after removing duplicates:", unique_rows, "\n")
# Print the rows being removed
print(rows_removed)
original_rows
View(data)
View(data)
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("scraped_royalsociety_for_now.csv")
data <- read.csv("scraped_royalsociety_for_now.csv")
View(data)
View(data)
View(data)
?download.file
data <- read.csv("scraped_royalsociety_for_now.csv")
#iterate through data and download each file from the download link under the column pdf_download_link with a timer of 1 second between downloads, put them into the destination folder (the downloaded file) under the same name as the title column of that row
for (i in 1:nrow(data)) {
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb")
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#iterate through data and download each file from the download link under the column pdf_download_link with a timer of 1 second between downloads, put them into the destination folder (the downloaded file) under the same name as the title column of that row
#do something to bypass the http 403 status
for (i in 1:nrow(data)) {
print(data$pdf_download_link[i])
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb")
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#iterate through data and download each file from the download link under the column pdf_download_link with a timer of 1 second between downloads, put them into the destination folder (the downloaded file) under the same name as the title column of that row
#do something to bypass the http 403 status
for (i in 1:nrow(data)) {
print(data$pdf_download_link[i])
print(data$title[i])
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb")
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#iterate through data and download each file from the download link under the column pdf_download_link with a timer of 1 second between downloads, put them into the destination folder (the downloaded file) under the same name as the title column of that row
#maybe add more to the download name so it can also encompass date information as well as doi
for (i in 1:nrow(data)) {
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb", headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'))
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
?download.file
data <- read.csv("scraped_royalsociety_for_now.csv")
#add a user agent to the download.file function to avoid being blocked by the website
for (i in 1:nrow(data)) {
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb", headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'), headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36')
print(data$title[i] + " has been downloaded, maybe")
data <- read.csv("scraped_royalsociety_for_now.csv")
#add a user agent to the download.file function to avoid being blocked by the website
for (i in 1:nrow(data)) {
download.file(data$pdf_download_link[i], paste0("D:/Fact_fiction_corpus/", data$title[i]), mode = "wb", headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'), headers = c("User-Agent" = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'))
print(data$title[i] + " has been downloaded, maybe")
Sys.sleep(10)
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#add a user agent to the download.file function to avoid being blocked by the website
for (i in 1:nrow(data)) {
print(base::curlGetHeaders(data$pdf_download_link[i]))
}
data <- read.csv("scraped_royalsociety_for_now.csv")
#add a user agent to the download.file function to avoid being blocked by the website
for (i in 1:nrow(data)) {
print(base::curlGetHeaders(data$pdf_download_link[1]))
}
