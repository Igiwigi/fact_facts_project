{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape all the possible\n",
    "https://royalsocietypublishing.org/doi/10.1098/rstl.1809.0001\n",
    "links (number combinations after rstl)\n",
    "(change the numbers, program reasonable ones to cycle through)\n",
    "(1600 to 1899 or 1600 to 1886)\n",
    "(0000 to 0100)\n",
    "\n",
    "add epdf to the found links to get the pdf itself, scrape it:\n",
    "title, author, publisher, year, content {as a long string}\n",
    "{can also maybe skip the 1st step and do the cycling to epdf}\n",
    "\n",
    "clean the text so it is non-punctuated (or search through it without caring ab punctuation)\n",
    "\n",
    "do post-processing on each scraped column to see if the content has words of interest, create a new dataframe with each row containing each instance + metadata + 30 word context\n",
    "\n",
    "remove exact duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDO \n",
    "# - make it so the crawler works and isnt immediately caught\n",
    "# - make it so that it can see all available URLS through only checking the HEAD before moving on to making a request (if brute forcing)\n",
    "# - make it so that the crawler downloads the pdf locally, as well as the html of the pre-website (to get the metadata)\n",
    "# - give the crawler rules and test how a CrawlerSpider works\n",
    "# link = response.xpath('//a[contains(@class, \"next-posts-link\")]/a/@href').get() --extract the link forward?\n",
    "\n",
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class RoyalSocietySpider(CrawlSpider):\n",
    "    name = 'royalsociety_spider'\n",
    "    allowed_domains = ['royalsocietypublishing.org']\n",
    "    start_urls = ['https://royalsocietypublishing.org/loi/rstl/group/c1600.d1690.y1692']\n",
    "\n",
    "    custom_settings = {\n",
    "    # Where the item data should be saved\n",
    "    'FEEDS': {'royalsociety.csv': {'format': 'csv', 'overwrite': True}},\n",
    "    \n",
    "    # Headers: This might have to be site specific to get past the 403 scraping measures\n",
    "    'HEADERS': {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:101.0) Gecko/20100101 Firefox/101.0',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'DNT': '1',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'If-None-Match': '\"f0267-ybK8wNq/yADu0m5N1CYhPqrXfaY\"',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    rules = (\n",
    "        #stay and follow links on rstl, dont go to journals or for-readers\n",
    "        Rule(LinkExtractor(allow=(r\"rstl\",), deny=(r\"journals\", r\"/rstl/for-readers\"))),\n",
    "        # when a link is the pre-part of an article, extract it, do a specific parse from which the pdf will also be gained\n",
    "        Rule(LinkExtractor(allow=(r\"g/doi/10.1098/rstl\",)), callback=\"parse_intro_page\"),\n",
    "    )\n",
    "\n",
    "    def parse_intro_page(self, response):\n",
    "        self.logger.info(\"Hi, this is an article page! %s\", response.url)\n",
    "\n",
    "        item = scrapy.Item() #creating an item\n",
    "\n",
    "        #scraping all the info from the article page and placing it in an item\n",
    "        item['author'] = response.css('a.author-name::attr(title)').get()\n",
    "        item['author'] = item['author'] if item['author'] else 'Unknown Author' #if author doesnt exist, its unknown author\n",
    "\n",
    "        item['publisher'] = response.css('meta[name=\"dc.Publisher\"]::attr(content)').get()\n",
    "        item['date'] = response.css('meta[name=\"dc.Date\"]::attr(content)').get()\n",
    "        item['identifier'] = response.css('meta[name=\"dc.Identifier\"]::attr(content)').get()\n",
    "        item['og_url'] = response.css('meta[property=\"og:url\"]::attr(content)').get()\n",
    "        item['title'] = response.css('meta[name=\"dc.Title\"]::attr(content)').get()\n",
    "        item['language'] = response.css('meta[name=\"dc.Language\"]::attr(content)').get()\n",
    "        item['pdf_link'] = response.css('a:contains(\"View PDF\")::attr(href)').get() #gets the pdf link\n",
    "\n",
    "        #save the start pages with their url and title as html, just for now\n",
    "        if item['og_url']: \n",
    "            og_url_without_quotes = item['og_url'].replace(\"'\", \"\")\n",
    "            title = item['title']\n",
    "            filename = f\"{title}_{og_url_without_quotes.split('/')[-1]}.html\"\n",
    "\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.body)\n",
    "                self.log(f'Saved file {filename}')\n",
    "        else:\n",
    "            self.log('No og:url found in the item; not saved')\n",
    "\n",
    "        yield item\n",
    "\n",
    "        if item['pdf_link'] is not None: #if there is a link to the pdf, follow it\n",
    "            return response.follow(\n",
    "                item['pdf_link'], self.parse_pdf, cb_kwargs=dict(item=item)\n",
    "            )\n",
    "\n",
    "    def parse_pdf(self, response, item):\n",
    "        #download pdf https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1809.0001?download=true add ?download=true to the link\n",
    "        pdf_url = item['pdf_link']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 10:02:43 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-04-09 10:02:43 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.9.1, w3lib 2.1.2, Twisted 24.3.0, Python 3.9.10 (tags/v3.9.10:f2f3f53, Jan 17 2022, 15:14:21) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.1.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.5, Platform Windows-10-10.0.22631-SP0\n",
      "2024-04-09 10:02:43 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-04-09 10:02:43 [py.warnings] WARNING: c:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-04-09 10:02:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-04-09 10:02:43 [scrapy.extensions.telnet] INFO: Telnet Password: d933cc195e33628f\n",
      "2024-04-09 10:02:43 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-04-09 10:02:43 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-04-09 10:02:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-04-09 10:02:43 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-04-09 10:02:43 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-04-09 10:02:43 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-04-09 10:02:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-04-09 10:02:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-04-09 10:02:44 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://royalsocietypublishing.org/loi/rstl/group/c1600.d1690.y1692> (referer: None)\n",
      "2024-04-09 10:02:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://royalsocietypublishing.org/loi/rstl/group/c1600.d1690.y1692>: HTTP status code is not handled or not allowed\n",
      "2024-04-09 10:02:44 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-04-09 10:02:44 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data.csv\n",
      "2024-04-09 10:02:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 259,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 9246,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'elapsed_time_seconds': 0.342097,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 4, 9, 8, 2, 44, 259323, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 16465,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 12,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2024, 4, 9, 8, 2, 43, 917226, tzinfo=datetime.timezone.utc)}\n",
      "2024-04-09 10:02:44 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "# Initialize CrawlerProcess\n",
    "process = CrawlerProcess()\n",
    "\n",
    "# Start the spider\n",
    "process.crawl(RoyalSocietySpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the logic (this is brute forcing though)\n",
    "year_range = [1600, 1899]\n",
    "issue_range = [0, 100]\n",
    "for year in range(year_range[0], year_range[1] + 1):\n",
    "            for issue in range(issue_range[0], issue_range[1] + 1):\n",
    "                issue_formatted = str(issue).zfill(4)\n",
    "                url = f'https://royalsocietypublishing.org/doi/epdf/10.1098/rstl.{year}.{issue_formatted}'\n",
    "                print(url)  # Print the generated URL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
