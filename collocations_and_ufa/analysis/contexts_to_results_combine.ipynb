{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to make the contexts match the found collocates so I can read them later for qualitative analysis.\n",
    "\n",
    "Slightly messy and verbose + AI'd to the wazoo, but it functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data loaded:\n",
      "Rows with collocate 'very': 2\n",
      "Total rows: 358\n",
      "\n",
      "Found 285 context files\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_collocates(input_csv, context_directory, output_csv=None, debug=False):\n",
    "    \"\"\"\n",
    "    Process collocate data by merging with context files and removing duplicates.\n",
    "   \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_csv : str\n",
    "        Path to the input CSV file containing collocate results\n",
    "    context_directory : str\n",
    "        Path to the directory containing context CSV files\n",
    "    output_csv : str, optional\n",
    "        Path where the output CSV should be saved. If None, no file is saved\n",
    "    debug : bool, optional\n",
    "        If True, print debugging information during processing\n",
    "       \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed DataFrame with merged context data and duplicates removed\n",
    "    \"\"\"\n",
    "    def print_debug(message, df=None, collocate=None):\n",
    "        if debug:\n",
    "            print(message)\n",
    "            if df is not None and collocate:\n",
    "                print(f\"Rows with collocate '{collocate}': {len(df[df['Collocate'] == collocate])}\")\n",
    "            if df is not None:\n",
    "                print(f\"Total rows: {len(df)}\\n\")\n",
    "   \n",
    "    # Step 1: Load the primary DataFrame and drop \"Unnamed\" columns\n",
    "    r1 = pd.read_csv(input_csv)\n",
    "    r1 = r1.loc[:, ~r1.columns.str.contains('^Unnamed')]\n",
    "    print_debug(\"Initial data loaded:\", r1, \"very\")\n",
    "    \n",
    "    # Step 2: Read all context files and combine them\n",
    "    context_files = [f for f in os.listdir(context_directory) if f.endswith('.csv')]\n",
    "    print_debug(f\"Found {len(context_files)} context files\")\n",
    "   \n",
    "    # Load all context files into one DataFrame\n",
    "    context_data = pd.DataFrame()\n",
    "    for file in context_files:\n",
    "        file_path = os.path.join(context_directory, file)\n",
    "        context_temp = pd.read_csv(file_path)\n",
    "        context_temp = context_temp.loc[:, ~context_temp.columns.str.contains('^Unnamed')]\n",
    "        context_data = pd.concat([context_data, context_temp], ignore_index=True)\n",
    "    print_debug(\"Context data loaded:\", context_data, \"very\")\n",
    "    \n",
    "    # Step 3: Merge DataFrames\n",
    "    merged_df = pd.merge(r1, context_data, left_on=['Year', 'Collocate'],\n",
    "                        right_on=['Year', 'Collocate'], how='left')\n",
    "    print_debug(\"After merge, step 3:\", merged_df, \"very\")\n",
    "    \n",
    "    # Step 4: Remove consecutive identical rows\n",
    "    merged_df['next_year'] = merged_df['Year'].shift(-1)\n",
    "    columns_to_check = [col for col in merged_df.columns if col not in ['Year']]\n",
    "    mask_consecutive_identical = (\n",
    "        (merged_df['next_year'] == merged_df['Year']) &\n",
    "        merged_df[columns_to_check].eq(merged_df[columns_to_check].shift(-1), axis=0).all(axis=1)\n",
    "    )\n",
    "   \n",
    "    merged_df = merged_df[~mask_consecutive_identical]\n",
    "    merged_df.drop(columns=['next_year'], inplace=True)\n",
    "    print_debug(\"After removing consecutive duplicates, step 4:\", merged_df, \"very\")\n",
    "    \n",
    "    # Step 5: Remove exact duplicates\n",
    "    merged_df.drop_duplicates(inplace=True)\n",
    "    print_debug(\"After removing exact duplicates, step 5:\", merged_df, \"very\")\n",
    "    \n",
    "    # Step 6: Remove duplicates keeping lowest year\n",
    "    columns_to_compare = [col for col in merged_df.columns if col != 'Year']\n",
    "    merged_df.sort_values(by='Year', inplace=True)\n",
    "    merged_df.drop_duplicates(subset=columns_to_compare, keep='first', inplace=True)\n",
    "    print_debug(\"After removing duplicates (keeping lowest year), step 6:\", merged_df, \"very\")\n",
    "    \n",
    "    # Step 7: Remove duplicates excluding MI Score and Year #this removes some data that it shouldnt\n",
    "    #columns_to_compare_mi = [col for col in merged_df.columns if col not in ['Year', 'MI Score']]\n",
    "    #merged_df.sort_values(by='Year', inplace=True)\n",
    "    #merged_df.drop_duplicates(subset=columns_to_compare_mi, keep='first', inplace=True)\n",
    "    #print_debug(\"After removing duplicates (excluding MI Score):\", merged_df, \"very\")\n",
    "    \n",
    "    # Step 8: Reset index\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Save to CSV if output path is provided\n",
    "    if output_csv:\n",
    "        merged_df.to_csv(output_csv, index=False)\n",
    "        print_debug(f\"Saved to {output_csv}\")\n",
    "   \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "result_df1 = process_collocates(\n",
    "    'collocate_results/dfs/fact3.csv',\n",
    "    \"../collocation_results/FACT/collocate_results_1665-1958_FACT_css3_w3/contexts\",\n",
    "    'collocate_results/contexts/fact3_contexts.csv',\n",
    "    debug=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df1 = process_collocates(\n",
    "    'collocate_results/dfs/fact3.csv',\n",
    "    \"../collocation_results/FACT/collocate_results_1665-1958_FACT_css3_w3/contexts\",\n",
    "    'collocate_results/contexts/fact3_contexts.csv',\n",
    "    debug=True \n",
    ")\n",
    "\n",
    "result_df2 = process_collocates(\n",
    "    'collocate_results/dfs/fact5.csv',\n",
    "    \"../collocation_results/FACT/collocate_results_1665-1958_FACT_css3_w5/contexts\",\n",
    "    'collocate_results/contexts/window5/fact5_contexts.csv',\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "result_df3 = process_collocates(\n",
    "    'collocate_results/dfs/facts3.csv',\n",
    "    \"../collocation_results/FACTS/collocate_results_1665-1958_FACTS_css3_w3/contexts\",\n",
    "    'collocate_results/contexts/facts3_contexts.csv',\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "result_df4 = process_collocates(\n",
    "    'collocate_results/dfs/facts5.csv',\n",
    "    \"../collocation_results/FACTS/collocate_results_1665-1958_FACTS_css3_w5/contexts\",\n",
    "    'collocate_results/contexts/window5/facts5_contexts.csv',\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing & other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Collocate  MI Score  \\\n",
      "0  1670    matter      7.99   \n",
      "1  1675    matter      8.09   \n",
      "2  1685    matter      8.30   \n",
      "3  1686    matter      8.12   \n",
      "4  1694    matter      7.46   \n",
      "\n",
      "                                             Context                File  \\\n",
      "0                                                NaN                 NaN   \n",
      "1  irrational, if the matter of fact be true ; fo...  rstl_1675_0037.txt   \n",
      "2  Philosophy , trill matter of fact and experime...  rstl_1685_0041.txt   \n",
      "3  uncertainty, but the matter of FaCt be- ing mo...  rstl_1686_0017.txt   \n",
      "4  now put the Matter of Fact out of Question, wh...  rstl_1694_0041.txt   \n",
      "\n",
      "  Directory  \n",
      "0       NaN  \n",
      "1  txt_rstl  \n",
      "2  txt_rstl  \n",
      "3  txt_rstl  \n",
      "4  txt_rstl  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Load the primary DataFrame and drop \"Unnamed\" columns (if any)\n",
    "r1 = pd.read_csv('collocate_results/dfs/fact5.csv')\n",
    "r1 = r1.loc[:, ~r1.columns.str.contains('^Unnamed')]  # Drop any column with 'Unnamed' in the name\n",
    "\n",
    "# Step 2: Read all context files and combine them into a single DataFrame\n",
    "context_dir =     \"../collocation_results/FACT/collocate_results_1665-1958_FACT_css3_w5/contexts\"\n",
    "context_files = [f for f in os.listdir(context_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Load all context files into one DataFrame\n",
    "context_data = pd.DataFrame()\n",
    "for file in context_files:\n",
    "    file_path = os.path.join(context_dir, file)\n",
    "    context_temp = pd.read_csv(file_path)\n",
    "    context_temp = context_temp.loc[:, ~context_temp.columns.str.contains('^Unnamed')]  # Drop \"Unnamed\" columns here as well\n",
    "    context_data = pd.concat([context_data, context_temp], ignore_index=True)\n",
    "\n",
    "# Step 3: Merge the primary DataFrame (r1) with the context data\n",
    "merged_df = pd.merge(r1, context_data, left_on=['Year', 'Collocate'], right_on=['Year', 'Collocate'], how='left')\n",
    "\n",
    "# Step 4: Handle NaN values\n",
    "# Fill NaN values in 'Context', 'File', and 'Directory' with the previous valid value (this is later removed so a little arbitrary, but its so I can view them anyway, the differing MI Scores, if i want to)\n",
    "merged_df[['Context', 'File', 'Directory']] = merged_df[['Context', 'File', 'Directory']].fillna(method='ffill')\n",
    "\n",
    "# Step 5: Remove consecutive rows with identical values (excluding 'Year')\n",
    "# Create a mask for rows where all columns except 'Year' are identical to the next row\n",
    "merged_df['next_year'] = merged_df['Year'].shift(-1)\n",
    "columns_to_check = [col for col in merged_df.columns if col not in ['Year']]  # Exclude 'Year'\n",
    "\n",
    "# Compare each row with the next one, considering all columns except 'Year'\n",
    "mask_consecutive_identical = (\n",
    "    (merged_df['next_year'] == merged_df['Year']) &\n",
    "    merged_df[columns_to_check].eq(merged_df[columns_to_check].shift(-1), axis=0).all(axis=1)\n",
    ")\n",
    "\n",
    "# Remove rows where consecutive years have identical values\n",
    "merged_df = merged_df[~mask_consecutive_identical]\n",
    "\n",
    "# Drop the temporary 'next_year' column\n",
    "merged_df.drop(columns=['next_year'], inplace=True)\n",
    "\n",
    "# Step 6: Remove exact duplicates (where all columns are identical)\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Step 7: Remove duplicates where all columns except 'Year' are the same, keeping the lowest 'Year' (same document and collocates, just slightly different years)\n",
    "# First, exclude 'Year' from the columns for comparison\n",
    "columns_to_compare = [col for col in merged_df.columns if col != 'Year']\n",
    "\n",
    "# Sort the dataframe by 'Year' to ensure the lowest year comes first\n",
    "merged_df.sort_values(by='Year', inplace=True)\n",
    "\n",
    "# Drop duplicates based on all columns except 'Year', keeping the first (lowest 'Year')\n",
    "merged_df.drop_duplicates(subset=columns_to_compare, keep='first', inplace=True)\n",
    "\n",
    "# Step 8: Remove duplicates where all columns except 'MI Score' and 'Year' are the same, keeping the lowest 'Year' (same document and collocates, just slightly different years)\n",
    "# Exclude 'MI Score' and 'Year' from the columns for comparison\n",
    "columns_to_compare_mi = [col for col in merged_df.columns if col not in ['Year', 'MI Score']]\n",
    "\n",
    "# Sort the dataframe by 'Year' again to ensure the lowest year comes first\n",
    "merged_df.sort_values(by='Year', inplace=True)\n",
    "\n",
    "# Drop duplicates based on all columns except 'MI Score' and 'Year', keeping the first (lowest 'Year') (same document and collocates, just slightly different years)\n",
    "merged_df.drop_duplicates(subset=columns_to_compare_mi, keep='first', inplace=True)\n",
    "\n",
    "# Step 9: Reset the index\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(merged_df.head())\n",
    "merged_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: collocate_results/contexts/fact3_contexts.csv\n",
      "Collocates: ['of', 'matter', 'this', 'that', 'curious', 'new', 'interesting', 'important', 'remarkable', 'in', 'they']\n",
      "\n",
      "File: collocate_results/contexts/facts3_contexts.csv\n",
      "Collocates: ['which', 'these', 'are', 'have', 'some', 'other', 'new', 'many', 'appear', 'number', 'all', 'curious', 'respecting', 'following', 'i', 'stated', 'connected', 'recorded']\n",
      "\n",
      "File: collocate_results/contexts/window5/fact5_contexts.csv\n",
      "Collocates: ['matter', 'of', 'this', 'general', 'curious', 'new', 'interesting', 'remarkable', 'important', 'that', 'due', 'attention', 'owing', 'lies', 'view', 'spite', 'do']\n",
      "\n",
      "File: collocate_results/contexts/window5/facts5_contexts.csv\n",
      "Collocates: ['these', 'have', 'some', 'prove', 'new', 'stated', 'appear', 'preceding', 'many', 'general', 'ascertained', 'chemical', 'number', 'few', 'curious', 'respecting', 'following', 'paper', 'appears', 'such', 'recorded', 'connected', 'observed', 'known', 'now', 'all']\n"
     ]
    }
   ],
   "source": [
    "def get_unique_collocates(file):\n",
    "    collocates = pd.read_csv(file)['Collocate'].unique().tolist()\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"Collocates: {collocates}\")\n",
    "\n",
    "# Process each file\n",
    "get_unique_collocates('collocate_results/contexts/fact3_contexts.csv')\n",
    "get_unique_collocates('collocate_results/contexts/facts3_contexts.csv')\n",
    "\n",
    "get_unique_collocates('collocate_results/contexts/window5/fact5_contexts.csv')\n",
    "get_unique_collocates('collocate_results/contexts/window5/facts5_contexts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
