{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix:\n",
    "- make sure the code isnt bullshit (since chatgpt etc)\n",
    "- maybe use a better measure than pmi?\n",
    "- consider combining the rstb and rstl datasets (do many versions? one with all, 3 separate and one with rstl and rstb combined?)\n",
    "- maybe make the freq-filter vary across years since some will have less (MAYBE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING CONTEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text processing...\n",
      "Reading text files...\n",
      "Text files read and organized by year.\n",
      "Analyzing collocates by sliding window...\n",
      "Years 1887-1891: 243 collocates found\n",
      "Years 1888-1892: 214 collocates found\n",
      "Years 1889-1893: 237 collocates found\n",
      "Years 1890-1894: 320 collocates found\n",
      "Years 1891-1895: 359 collocates found\n",
      "Years 1892-1896: 337 collocates found\n",
      "Years 1893-1897: 357 collocates found\n",
      "Years 1894-1898: 352 collocates found\n",
      "Years 1895-1899: 264 collocates found\n",
      "Years 1896-1900: 258 collocates found\n",
      "Years 1897-1901: 253 collocates found\n",
      "Years 1898-1902: 180 collocates found\n",
      "Years 1899-1903: 145 collocates found\n",
      "Years 1900-1904: 189 collocates found\n",
      "Years 1901-1905: 158 collocates found\n",
      "Years 1902-1906: 181 collocates found\n",
      "Years 1903-1907: 181 collocates found\n",
      "Years 1904-1908: 183 collocates found\n",
      "Years 1905-1909: 190 collocates found\n",
      "Years 1906-1910: 157 collocates found\n",
      "Years 1907-1911: 124 collocates found\n",
      "Years 1908-1912: 160 collocates found\n",
      "Years 1909-1913: 138 collocates found\n",
      "Years 1910-1914: 160 collocates found\n",
      "Years 1911-1915: 196 collocates found\n",
      "Years 1912-1916: 203 collocates found\n",
      "Years 1913-1917: 170 collocates found\n",
      "Years 1914-1918: 172 collocates found\n",
      "Years 1915-1919: 71 collocates found\n",
      "Years 1916-1920: 83 collocates found\n",
      "Years 1917-1921: 90 collocates found\n",
      "Years 1918-1922: 90 collocates found\n",
      "Years 1919-1923: 103 collocates found\n",
      "Years 1920-1924: 123 collocates found\n",
      "Years 1921-1925: 121 collocates found\n",
      "Years 1922-1926: 107 collocates found\n",
      "Years 1923-1927: 150 collocates found\n",
      "Years 1924-1928: 145 collocates found\n",
      "Years 1925-1929: 170 collocates found\n",
      "Years 1926-1930: 164 collocates found\n",
      "Years 1927-1931: 214 collocates found\n",
      "Years 1928-1932: 257 collocates found\n",
      "Years 1929-1933: 227 collocates found\n",
      "Years 1930-1934: 242 collocates found\n",
      "Years 1931-1935: 244 collocates found\n",
      "Years 1932-1936: 193 collocates found\n",
      "Years 1933-1937: 208 collocates found\n",
      "Years 1934-1938: 244 collocates found\n",
      "Years 1935-1939: 196 collocates found\n",
      "Years 1936-1940: 174 collocates found\n",
      "Years 1937-1941: 145 collocates found\n",
      "Years 1938-1942: 80 collocates found\n",
      "Years 1939-1943: 52 collocates found\n",
      "Years 1940-1944: 28 collocates found\n",
      "Years 1941-1945: 23 collocates found\n",
      "Years 1942-1946: 26 collocates found\n",
      "Years 1943-1947: 62 collocates found\n",
      "Years 1944-1948: 75 collocates found\n",
      "Years 1945-1949: 89 collocates found\n",
      "Years 1946-1950: 125 collocates found\n",
      "Years 1947-1951: 165 collocates found\n",
      "Years 1948-1952: 139 collocates found\n",
      "Years 1949-1953: 158 collocates found\n",
      "Years 1950-1954: 144 collocates found\n",
      "Years 1951-1955: 107 collocates found\n",
      "Years 1952-1956: 57 collocates found\n",
      "Years 1953-1957: 27 collocates found\n",
      "Years 1954-1958: 2 collocates found\n",
      "Years 1955-1959: 0 collocates found\n",
      "Years 1956-1960: 0 collocates found\n",
      "Years 1957-1961: 0 collocates found\n",
      "Years 1958-1962: 0 collocates found\n",
      "Years 1959-1963: 0 collocates found\n",
      "Years 1960-1964: 0 collocates found\n",
      "Years 1961-1965: 0 collocates found\n",
      "Years 1962-1966: 0 collocates found\n",
      "Years 1963-1967: 0 collocates found\n",
      "Years 1964-1968: 0 collocates found\n",
      "Years 1965-1969: 0 collocates found\n",
      "Years 1966-1970: 0 collocates found\n",
      "Years 1967-1971: 0 collocates found\n",
      "Years 1968-1972: 0 collocates found\n",
      "Years 1969-1973: 0 collocates found\n",
      "Years 1970-1974: 0 collocates found\n",
      "Years 1971-1975: 0 collocates found\n",
      "Years 1972-1976: 0 collocates found\n",
      "Years 1973-1977: 0 collocates found\n",
      "Years 1974-1978: 0 collocates found\n",
      "Years 1975-1979: 0 collocates found\n",
      "Years 1976-1980: 0 collocates found\n",
      "Years 1977-1981: 0 collocates found\n",
      "Years 1978-1982: 0 collocates found\n",
      "Years 1979-1983: 0 collocates found\n",
      "Years 1980-1984: 0 collocates found\n",
      "Years 1981-1985: 0 collocates found\n",
      "Years 1982-1986: 0 collocates found\n",
      "Years 1983-1987: 0 collocates found\n",
      "Years 1984-1988: 0 collocates found\n",
      "Years 1985-1989: 0 collocates found\n",
      "Years 1986-1990: 0 collocates found\n",
      "Years 1987-1991: 0 collocates found\n",
      "Years 1988-1992: 0 collocates found\n",
      "Years 1989-1993: 0 collocates found\n",
      "Years 1990-1994: 0 collocates found\n",
      "Years 1991-1995: 0 collocates found\n",
      "Years 1992-1996: 0 collocates found\n",
      "Years 1993-1997: 0 collocates found\n",
      "Years 1994-1998: 0 collocates found\n",
      "Years 1995-1999: 0 collocates found\n",
      "Years 1996-2000: 1 collocates found\n",
      "Years 1997-2001: 2 collocates found\n",
      "Years 1998-2002: 2 collocates found\n",
      "Years 1999-2003: 2 collocates found\n",
      "Years 2000-2004: 2 collocates found\n",
      "Years 2001-2005: 1 collocates found\n",
      "Years 2002-2006: 5 collocates found\n",
      "Years 2003-2007: 10 collocates found\n",
      "Years 2004-2008: 29 collocates found\n",
      "Years 2005-2009: 59 collocates found\n",
      "Years 2006-2010: 103 collocates found\n",
      "Years 2007-2011: 117 collocates found\n",
      "Years 2008-2012: 157 collocates found\n",
      "Years 2009-2013: 288 collocates found\n",
      "Years 2010-2014: 431 collocates found\n",
      "Years 2011-2015: 627 collocates found\n",
      "Years 2012-2016: 816 collocates found\n",
      "Years 2013-2017: 984 collocates found\n",
      "Years 2014-2018: 948 collocates found\n",
      "Years 2015-2019: 1087 collocates found\n",
      "Years 2016-2020: 1082 collocates found\n",
      "Years 2017-2021: 1089 collocates found\n",
      "Years 2018-2022: 1002 collocates found\n",
      "Years 2019-2023: 896 collocates found\n",
      "Years 2020-2024: 542 collocates found\n",
      "Saving results to CSV files...\n",
      "Collocate analysis completed. Results saved in the 'collocate_results_rstb' directory.\n",
      "Contexts saved to separate files for each window in the 'collocate_results_rstb' directory.\n"
     ]
    }
   ],
   "source": [
    "#simple but works, very slow though, need to make it a proper set of functions\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "text_dir = r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\"\n",
    "\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r'rst[bl]?_(\\d{4})', filename)\n",
    "    if not match:\n",
    "        match = re.search(r'rst[bl]?(\\d{4})', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def read_texts_by_year(directory):\n",
    "    texts_by_year = defaultdict(list)\n",
    "    print(\"Reading text files...\")\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            year = extract_year_from_filename(filename)\n",
    "            if year:\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                texts_by_year[year].append(text)\n",
    "    print(\"Text files read and organized by year.\")\n",
    "    return texts_by_year\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "def get_collocates_by_sliding_window(texts_by_year, word_of_interest, window_size=5):\n",
    "    collocates_per_window = {}\n",
    "    all_years = sorted(texts_by_year.keys())\n",
    "    start_year = min(all_years)\n",
    "    end_year = max(all_years)\n",
    "\n",
    "    print(\"Analyzing collocates by sliding window...\")\n",
    "    for window_start in range(start_year, end_year - window_size + 2):\n",
    "        window_end = window_start + window_size - 1\n",
    "        window_texts = []\n",
    "        for year in range(window_start, window_end + 1):\n",
    "            if year in texts_by_year:\n",
    "                window_texts.extend(texts_by_year[year])\n",
    "        \n",
    "        all_tokens = []\n",
    "        for text in window_texts:\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [token.lower() for token in tokens if token not in string.punctuation and token.lower() not in stop_words and not token.isdigit()]\n",
    "            all_tokens.extend(tokens)\n",
    "        \n",
    "        tokens = lemmatize_tokens(all_tokens)\n",
    "        \n",
    "        total_tokens = len(tokens)\n",
    "        total_texts = len(window_texts)\n",
    "        \n",
    "        bigram_measures = BigramAssocMeasures()\n",
    "        finder = BigramCollocationFinder.from_words(tokens)\n",
    "        finder.apply_freq_filter(4)\n",
    "        \n",
    "        collocations = finder.score_ngrams(bigram_measures.pmi)\n",
    "        \n",
    "        collocate_stats = []\n",
    "        collocate_contexts = {}\n",
    "\n",
    "        for bigram, pmi in collocations:\n",
    "            if word_of_interest in bigram:\n",
    "                other_word = bigram[0] if bigram[1] == word_of_interest else bigram[1]\n",
    "                observed_freq = finder.ngram_fd[bigram]\n",
    "                word_freq = finder.word_fd[other_word]\n",
    "                expected_freq = (finder.word_fd[word_of_interest] * word_freq) / total_tokens\n",
    "                num_texts = sum(1 for text in window_texts if other_word in text.split())\n",
    "                \n",
    "                collocate_stats.append({\n",
    "                    'word': other_word,\n",
    "                    'total_corpus': word_freq,\n",
    "                    'expected_freq': expected_freq,\n",
    "                    'observed_freq': observed_freq,\n",
    "                    'num_texts': num_texts,\n",
    "                    'pmi': pmi\n",
    "                })\n",
    "                \n",
    "                # Store contexts for this collocate\n",
    "                collocate_contexts[other_word] = []\n",
    "                for text in window_texts:\n",
    "                    words = text.split()\n",
    "                    for i, word in enumerate(words):\n",
    "                        if word == word_of_interest and other_word in words[max(0, i-5):i+6]:\n",
    "                            context = ' '.join(words[max(0, i-5):i+6])\n",
    "                            collocate_contexts[other_word].append((f\"Years {window_start}-{window_end}: {context}\", other_word))\n",
    "        \n",
    "        collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "        top_collocates = collocate_stats[:10]\n",
    "        \n",
    "        # Only keep contexts for top collocates\n",
    "        filtered_contexts = []\n",
    "        for collocate in top_collocates:\n",
    "            filtered_contexts.extend(collocate_contexts.get(collocate['word'], []))\n",
    "        \n",
    "        collocates_per_window[f\"{window_start}-{window_end}\"] = {\n",
    "            'collocates': top_collocates,\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_texts': total_texts,\n",
    "            'contexts': filtered_contexts\n",
    "        }\n",
    "        print(f\"Years {window_start}-{window_end}: {len(top_collocates)} collocates found\")\n",
    "\n",
    "    return collocates_per_window\n",
    "\n",
    "print(\"Starting text processing...\")\n",
    "texts_by_year = read_texts_by_year(text_dir)\n",
    "collocates_by_window = get_collocates_by_sliding_window(texts_by_year, \"fact\")\n",
    "\n",
    "# Saving collocation results to CSV files\n",
    "output_dir = 'collocate_results_rstb'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"Saving results to CSV files...\")\n",
    "\n",
    "for window, data in collocates_by_window.items():\n",
    "    filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                         'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "        \n",
    "        for i, collocate in enumerate(data['collocates'], 1):\n",
    "            writer.writerow([\n",
    "                i,\n",
    "                collocate['word'],\n",
    "                collocate['total_corpus'],\n",
    "                f\"{collocate['expected_freq']:.2f}\",\n",
    "                collocate['observed_freq'],\n",
    "                collocate['num_texts'],\n",
    "                f\"{collocate['pmi']:.2f}\"\n",
    "            ])\n",
    "\n",
    "    # Saving collocate contexts to a CSV file for each window\n",
    "    context_file = os.path.join(output_dir, f'collocate_contexts_{window}.csv')\n",
    "    with open(context_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Context', 'Collocate'])\n",
    "        for context, collocate in data['contexts']:\n",
    "            writer.writerow([context, collocate])\n",
    "\n",
    "print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "print(f\"Contexts saved to separate CSV files for each window in the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matter' 'theſe' 'known' 'many' 'important' 'respecting' 'new'\n",
      " 'following' 'state' 'case' 'however' 'prove' 'stated' 'experiment'\n",
      " 'preceding' 'ascertained' 'appear' 'general' 'adduced' 'chemical'\n",
      " 'series' 'may' 'number' 'observation' 'curious' 'observed' 'well'\n",
      " 'appears' 'singular' 'importance' 'propriety' 'certainty' 'noticed'\n",
      " 'principal' 'connected' 'interesting' 'proved' 'remarkable' 'might'\n",
      " 'induction' 'accordance' 'recorded' 'knowledge' 'additional' 'foregoing'\n",
      " 'mentioned' 'striking' 'shown' 'induced' 'already' 'point' 'described'\n",
      " 'explanation' 'establish' 'penetration' 'supported' 'pointed' 'express'\n",
      " 'established' 'existence' 'think' 'always' 'latter' 'system' 'made'\n",
      " 'attention' 'mere' 'quantic' 'seems' 'founded' 'evidence' 'account'\n",
      " 'evident' 'probable' 'explain' 'yet' 'due' 'anatomical' 'expression'\n",
      " 'arises' 'conic' 'upon' 'owing' 'bearing' 'explained' 'interest' 'partly'\n",
      " 'noted' 'statement' 'obseryed' 'confirmed' 'experimental' 'detailed'\n",
      " 'interpretation' 'mind' 'tends' 'consistent' 'relating' 'teleostei'\n",
      " 'thab' 'indicated' 'accounted' 'discovered' 'arrange' 'fundamental'\n",
      " 'view' 'spite' 'significant' 'demonstrated' 'mention' 'familiar'\n",
      " 'consequence' 'struck' 'lie' 'together' 'regard' 'brought' 'beyond'\n",
      " 'reveals' 'colour' 'two' 'apart' 'injection' 'seem' 'suggest' 'none'\n",
      " 'correlated' 'recognition' 'despite' 'whole' 'regarding' 'found' 'would'\n",
      " 'borne' 'exist' 'stele' 'larva' 'cell' 'concerning' 'specie' 'differ'\n",
      " 'specimen' 'one' 'al' 'stylized' 'change' 'reflect' 'certain' 'even'\n",
      " 'animal' 'complicated' 'reflecting' 'reflects' 'highlight' 'trier'\n",
      " 'sheet' 'relies' 'notwithstanding' 'hampered' 'relates' 'arithmetical'\n",
      " 'attributable' 'finder' 'exacerbated' 'virtue' 'arithmetic' 'fiction'\n",
      " 'undermined' 'underscore' 'aware']\n",
      "The keyword 'well' is found in the following document(s):\n",
      "old/collocate_results_combined_ff5\\collocates_1811-1815.csv\n",
      "old/collocate_results_combined_ff5\\collocates_1812-1816.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"old/collocate_results_combined_ff5\"\n",
    "\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "combined_data = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "unique_words = combined_data['Word'].unique()\n",
    "print(unique_words)\n",
    "\n",
    "keyword = \"well\"\n",
    "keyword_docs = []\n",
    "\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if keyword in df['Word'].values:\n",
    "        keyword_docs.append(f)\n",
    "\n",
    "if keyword_docs:\n",
    "    print(f\"The keyword '{keyword}' is found in the following document(s):\")\n",
    "    for doc in keyword_docs:\n",
    "        print(doc)\n",
    "else:\n",
    "    print(f\"The keyword '{keyword}' is not found in any document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text processing...\n",
      "Reading text files...\n",
      "Text files read and organized by year.\n",
      "Analyzing collocates by sliding window...\n",
      "Years 1665-1669: 0 collocates found\n",
      "Years 1666-1670: 0 collocates found\n",
      "Years 1667-1671: 0 collocates found\n",
      "Years 1668-1672: 0 collocates found\n",
      "Years 1669-1673: 1 collocates found\n",
      "Years 1670-1674: 1 collocates found\n",
      "Years 1671-1675: 1 collocates found\n",
      "Years 1672-1676: 1 collocates found\n",
      "Years 1673-1677: 1 collocates found\n",
      "Years 1674-1678: 0 collocates found\n",
      "Years 1675-1679: 0 collocates found\n",
      "Years 1676-1680: 0 collocates found\n",
      "Years 1677-1681: 0 collocates found\n",
      "Years 1678-1682: 0 collocates found\n",
      "Years 1679-1683: 0 collocates found\n",
      "Years 1680-1684: 0 collocates found\n",
      "Years 1681-1685: 1 collocates found\n",
      "Years 1682-1686: 1 collocates found\n",
      "Years 1683-1687: 1 collocates found\n",
      "Years 1684-1688: 1 collocates found\n",
      "Years 1685-1689: 1 collocates found\n",
      "Years 1686-1690: 0 collocates found\n",
      "Years 1687-1691: 0 collocates found\n",
      "Years 1688-1692: 0 collocates found\n",
      "Years 1689-1693: 0 collocates found\n",
      "Years 1690-1694: 0 collocates found\n",
      "Years 1691-1695: 0 collocates found\n",
      "Years 1692-1696: 0 collocates found\n",
      "Years 1693-1697: 0 collocates found\n",
      "Years 1694-1698: 0 collocates found\n",
      "Years 1695-1699: 0 collocates found\n",
      "Years 1696-1700: 0 collocates found\n",
      "Years 1697-1701: 0 collocates found\n",
      "Years 1698-1702: 0 collocates found\n",
      "Years 1699-1703: 0 collocates found\n",
      "Years 1700-1704: 0 collocates found\n",
      "Years 1701-1705: 0 collocates found\n",
      "Years 1702-1706: 0 collocates found\n",
      "Years 1703-1707: 0 collocates found\n",
      "Years 1704-1708: 1 collocates found\n",
      "Years 1705-1709: 1 collocates found\n",
      "Years 1706-1710: 1 collocates found\n",
      "Years 1707-1711: 0 collocates found\n",
      "Years 1708-1712: 0 collocates found\n",
      "Years 1709-1713: 0 collocates found\n",
      "Years 1710-1714: 0 collocates found\n",
      "Years 1711-1715: 0 collocates found\n",
      "Years 1712-1716: 0 collocates found\n",
      "Years 1713-1717: 0 collocates found\n",
      "Years 1714-1718: 0 collocates found\n",
      "Years 1715-1719: 0 collocates found\n",
      "Years 1716-1720: 0 collocates found\n",
      "Years 1717-1721: 0 collocates found\n",
      "Years 1718-1722: 0 collocates found\n",
      "Years 1719-1723: 0 collocates found\n",
      "Years 1720-1724: 0 collocates found\n",
      "Years 1721-1725: 0 collocates found\n",
      "Years 1722-1726: 0 collocates found\n",
      "Years 1723-1727: 0 collocates found\n",
      "Years 1724-1728: 0 collocates found\n",
      "Years 1725-1729: 0 collocates found\n",
      "Years 1726-1730: 0 collocates found\n",
      "Years 1727-1731: 0 collocates found\n",
      "Years 1728-1732: 0 collocates found\n",
      "Years 1729-1733: 0 collocates found\n",
      "Years 1730-1734: 0 collocates found\n",
      "Years 1731-1735: 0 collocates found\n",
      "Years 1732-1736: 0 collocates found\n",
      "Years 1733-1737: 0 collocates found\n",
      "Years 1734-1738: 0 collocates found\n",
      "Years 1735-1739: 0 collocates found\n",
      "Years 1736-1740: 0 collocates found\n",
      "Years 1737-1741: 0 collocates found\n",
      "Years 1738-1742: 0 collocates found\n",
      "Years 1739-1743: 0 collocates found\n",
      "Years 1740-1744: 0 collocates found\n",
      "Years 1741-1745: 0 collocates found\n",
      "Years 1742-1746: 0 collocates found\n",
      "Years 1743-1747: 0 collocates found\n",
      "Years 1744-1748: 0 collocates found\n",
      "Years 1745-1749: 0 collocates found\n",
      "Years 1746-1750: 0 collocates found\n",
      "Years 1747-1751: 0 collocates found\n",
      "Years 1748-1752: 0 collocates found\n",
      "Years 1749-1753: 0 collocates found\n",
      "Years 1750-1754: 0 collocates found\n",
      "Years 1751-1755: 0 collocates found\n",
      "Years 1752-1756: 0 collocates found\n",
      "Years 1753-1757: 0 collocates found\n",
      "Years 1754-1758: 0 collocates found\n",
      "Years 1755-1759: 0 collocates found\n",
      "Years 1756-1760: 0 collocates found\n",
      "Years 1757-1761: 0 collocates found\n",
      "Years 1758-1762: 0 collocates found\n",
      "Years 1759-1763: 0 collocates found\n",
      "Years 1760-1764: 0 collocates found\n",
      "Years 1761-1765: 0 collocates found\n",
      "Years 1762-1766: 0 collocates found\n",
      "Years 1763-1767: 0 collocates found\n",
      "Years 1764-1768: 0 collocates found\n",
      "Years 1765-1769: 0 collocates found\n",
      "Years 1766-1770: 0 collocates found\n",
      "Years 1767-1771: 0 collocates found\n",
      "Years 1768-1772: 0 collocates found\n",
      "Years 1769-1773: 0 collocates found\n",
      "Years 1770-1774: 0 collocates found\n",
      "Years 1771-1775: 0 collocates found\n",
      "Years 1772-1776: 0 collocates found\n",
      "Years 1773-1777: 0 collocates found\n",
      "Years 1774-1778: 0 collocates found\n",
      "Years 1775-1779: 0 collocates found\n",
      "Years 1776-1780: 0 collocates found\n",
      "Years 1777-1781: 0 collocates found\n",
      "Years 1778-1782: 0 collocates found\n",
      "Years 1779-1783: 0 collocates found\n",
      "Years 1780-1784: 0 collocates found\n",
      "Years 1781-1785: 0 collocates found\n",
      "Years 1782-1786: 0 collocates found\n",
      "Years 1783-1787: 0 collocates found\n",
      "Years 1784-1788: 0 collocates found\n",
      "Years 1785-1789: 0 collocates found\n",
      "Years 1786-1790: 0 collocates found\n",
      "Years 1787-1791: 0 collocates found\n",
      "Years 1788-1792: 0 collocates found\n",
      "Years 1789-1793: 0 collocates found\n",
      "Years 1790-1794: 0 collocates found\n",
      "Years 1791-1795: 1 collocates found\n",
      "Years 1792-1796: 2 collocates found\n",
      "Years 1793-1797: 2 collocates found\n",
      "Years 1794-1798: 3 collocates found\n",
      "Years 1795-1799: 2 collocates found\n",
      "Years 1796-1800: 2 collocates found\n",
      "Years 1797-1801: 2 collocates found\n",
      "Years 1798-1802: 1 collocates found\n",
      "Years 1799-1803: 3 collocates found\n",
      "Years 1800-1804: 4 collocates found\n",
      "Years 1801-1805: 6 collocates found\n",
      "Years 1802-1806: 8 collocates found\n",
      "Years 1803-1807: 8 collocates found\n",
      "Years 1804-1808: 12 collocates found\n",
      "Years 1805-1809: 11 collocates found\n",
      "Years 1806-1810: 6 collocates found\n",
      "Years 1807-1811: 7 collocates found\n",
      "Years 1808-1812: 3 collocates found\n",
      "Years 1809-1813: 3 collocates found\n",
      "Years 1810-1814: 6 collocates found\n",
      "Years 1811-1815: 7 collocates found\n",
      "Years 1812-1816: 6 collocates found\n",
      "Years 1813-1817: 4 collocates found\n",
      "Years 1814-1818: 5 collocates found\n",
      "Years 1815-1819: 5 collocates found\n",
      "Years 1816-1820: 3 collocates found\n",
      "Years 1817-1821: 6 collocates found\n",
      "Years 1818-1822: 6 collocates found\n",
      "Years 1819-1823: 7 collocates found\n",
      "Years 1820-1824: 9 collocates found\n",
      "Years 1821-1825: 10 collocates found\n",
      "Years 1822-1826: 9 collocates found\n",
      "Years 1823-1827: 8 collocates found\n",
      "Years 1824-1828: 8 collocates found\n",
      "Years 1825-1829: 9 collocates found\n",
      "Years 1826-1830: 6 collocates found\n",
      "Years 1827-1831: 4 collocates found\n",
      "Years 1828-1832: 4 collocates found\n",
      "Years 1829-1833: 14 collocates found\n",
      "Years 1830-1834: 15 collocates found\n",
      "Years 1831-1835: 18 collocates found\n",
      "Years 1832-1836: 27 collocates found\n",
      "Years 1833-1837: 24 collocates found\n",
      "Years 1834-1838: 18 collocates found\n",
      "Years 1835-1839: 15 collocates found\n",
      "Years 1836-1840: 29 collocates found\n",
      "Years 1837-1841: 24 collocates found\n",
      "Years 1838-1842: 21 collocates found\n",
      "Years 1839-1843: 18 collocates found\n",
      "Years 1840-1844: 16 collocates found\n",
      "Years 1841-1845: 6 collocates found\n",
      "Years 1842-1846: 8 collocates found\n",
      "Years 1843-1847: 8 collocates found\n",
      "Years 1844-1848: 12 collocates found\n",
      "Years 1845-1849: 11 collocates found\n",
      "Years 1846-1850: 19 collocates found\n",
      "Years 1847-1851: 24 collocates found\n",
      "Years 1848-1852: 25 collocates found\n",
      "Years 1849-1853: 39 collocates found\n",
      "Years 1850-1854: 37 collocates found\n",
      "Years 1851-1855: 19 collocates found\n",
      "Years 1852-1856: 16 collocates found\n",
      "Years 1853-1857: 16 collocates found\n",
      "Years 1854-1858: 24 collocates found\n",
      "Years 1855-1859: 39 collocates found\n",
      "Years 1856-1860: 42 collocates found\n",
      "Years 1857-1861: 45 collocates found\n",
      "Years 1858-1862: 51 collocates found\n",
      "Years 1859-1863: 34 collocates found\n",
      "Years 1860-1864: 32 collocates found\n",
      "Years 1861-1865: 30 collocates found\n",
      "Years 1862-1866: 26 collocates found\n",
      "Years 1863-1867: 24 collocates found\n",
      "Years 1864-1868: 25 collocates found\n",
      "Years 1865-1869: 25 collocates found\n",
      "Years 1866-1870: 24 collocates found\n",
      "Years 1867-1871: 23 collocates found\n",
      "Years 1868-1872: 22 collocates found\n",
      "Years 1869-1873: 24 collocates found\n",
      "Years 1870-1874: 20 collocates found\n",
      "Years 1871-1875: 34 collocates found\n",
      "Years 1872-1876: 32 collocates found\n",
      "Years 1873-1877: 43 collocates found\n",
      "Years 1874-1878: 34 collocates found\n",
      "Years 1875-1879: 36 collocates found\n",
      "Years 1876-1880: 43 collocates found\n",
      "Years 1877-1881: 41 collocates found\n",
      "Years 1878-1882: 47 collocates found\n",
      "Years 1879-1883: 63 collocates found\n",
      "Years 1880-1884: 57 collocates found\n",
      "Years 1881-1885: 42 collocates found\n",
      "Years 1882-1886: 49 collocates found\n",
      "Years 1883-1887: 33 collocates found\n",
      "Years 1884-1888: 26 collocates found\n",
      "Years 1885-1889: 27 collocates found\n",
      "Years 1886-1890: 21 collocates found\n",
      "Years 1887-1891: 25 collocates found\n",
      "Years 1888-1892: 23 collocates found\n",
      "Years 1889-1893: 29 collocates found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollocate analysis completed. Results saved in the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContexts saved to separate CSV files for each window in the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 161\u001b[0m \u001b[43manalyze_collocates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFact_fiction_corpus\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mroyal society\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtxt_rstb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFact_fiction_corpus\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mroyal society\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtxt_rstl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFact_fiction_corpus\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mroyal society\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtxt_rsta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollocate_results_combined_ff5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfact\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 128\u001b[0m, in \u001b[0;36manalyze_collocates\u001b[1;34m(text_dirs, output_dir, word_of_interest, window_size, collocate_window, freq_filter, top_n)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting text processing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m texts_by_year \u001b[38;5;241m=\u001b[39m read_texts_by_year(text_dirs)\n\u001b[1;32m--> 128\u001b[0m collocates_by_window \u001b[38;5;241m=\u001b[39m \u001b[43mget_collocates_by_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_by_year\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving results to CSV files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 104\u001b[0m, in \u001b[0;36manalyze_collocates.<locals>.get_collocates_by_sliding_window\u001b[1;34m(texts_by_year)\u001b[0m\n\u001b[0;32m    102\u001b[0m words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m word_of_interest_lower \u001b[38;5;129;01mand\u001b[39;00m other_word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words[\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, i\u001b[38;5;241m-\u001b[39mcollocate_window):i\u001b[38;5;241m+\u001b[39mcollocate_window\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]]:\n\u001b[0;32m    105\u001b[0m         context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words[\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, i\u001b[38;5;241m-\u001b[39mcollocate_window):i\u001b[38;5;241m+\u001b[39mcollocate_window\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    106\u001b[0m         collocate_contexts[other_word]\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYears \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, other_word))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_collocates(text_dirs, output_dir, word_of_interest, window_size=5, collocate_window=5, freq_filter=5, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyzes word collocates in text files over specified time windows and saves results to CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - text_dirs (list): List of directories containing text files.\n",
    "    - output_dir (str): Directory to save the output CSV files.\n",
    "    - word_of_interest (str): The word for which collocates are to be found.\n",
    "    - window_size (int): Number of years to include in each sliding window.\n",
    "    - collocate_window (int): Number of words around the word of interest to consider.\n",
    "    - freq_filter (int): Minimum frequency for including bigrams in the analysis.\n",
    "    - top_n (int): Number of top collocates to report.\n",
    "\n",
    "    Returns:\n",
    "    - None: Writes results to CSV files in the specified output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_year_from_filename(filename):\n",
    "        \"\"\" extracts year from filename, 2 patterns are supported: rstb_YYYY and rstbYYYY \"\"\"\n",
    "        match = re.search(r'rst[bl]?_(\\d{4})', filename)\n",
    "        if not match:\n",
    "            match = re.search(r'rst[bl]?(\\d{4})', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "\n",
    "    def read_texts_by_year(directories):\n",
    "        \"\"\" \n",
    "        reads text files from directories and organizes them by year \n",
    "        also fixes some small issues in the text files \n",
    "        \"\"\"\n",
    "        texts_by_year = defaultdict(list)\n",
    "        print(\"Reading text files...\")\n",
    "        for directory in directories:\n",
    "            for filename in os.listdir(directory):\n",
    "                if filename.endswith('.txt'):\n",
    "                    year = extract_year_from_filename(filename)\n",
    "                    if year:\n",
    "                        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                            text = file.read()\n",
    "                        text = text.replace('ſ', 's').replace('Å¿', 's') #these evade stopword detection otherwise\n",
    "                        text = text.replace('obseryed', 'observed')\n",
    "                        texts_by_year[year].append(text)\n",
    "        print(\"Text files read and organized by year.\")\n",
    "        return texts_by_year\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['al', \"thab\"]) #et al is also a stop word, thab is a typo of that\n",
    "    stop_words.update(['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']) #number words also shouldnt be included (at least not these)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_tokens(tokens):\n",
    "        \"\"\" lemmatizes tokens and removes non-alphabetic tokens \"\"\"\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "    def get_collocates_by_sliding_window(texts_by_year):\n",
    "        \"\"\" \n",
    "        finds collocates for the word of interest in sliding windows of text files\n",
    "        also makes each selected 5 word context into a file which can be perused later\n",
    "\n",
    "        currently using pmi as the measure!\n",
    "        \"\"\"\n",
    "        collocates_per_window = {}\n",
    "        all_years = sorted(texts_by_year.keys())\n",
    "        start_year = min(all_years)\n",
    "        end_year = max(all_years)\n",
    "\n",
    "        print(\"Analyzing collocates by sliding window...\")\n",
    "        for window_start in range(start_year, end_year - window_size + 2):\n",
    "            window_end = window_start + window_size - 1\n",
    "            window_texts = []\n",
    "            for year in range(window_start, window_end + 1):\n",
    "                if year in texts_by_year:\n",
    "                    window_texts.extend(texts_by_year[year])\n",
    "            \n",
    "            all_tokens = []\n",
    "            for text in window_texts:\n",
    "                tokens = word_tokenize(text)\n",
    "                tokens = [token.lower() for token in tokens if token not in string.punctuation and token.lower() not in stop_words and not token.isdigit()]\n",
    "                all_tokens.extend(tokens)\n",
    "            \n",
    "            tokens = lemmatize_tokens(all_tokens) #maybe should be done earlier\n",
    "            \n",
    "            word_of_interest_lower = word_of_interest.lower()\n",
    "            \n",
    "            total_tokens = len(tokens) #is this correctly taken for later comparison?\n",
    "            total_texts = len(window_texts)\n",
    "            \n",
    "            bigram_measures = BigramAssocMeasures()\n",
    "            finder = BigramCollocationFinder.from_words(tokens)\n",
    "            finder.apply_freq_filter(freq_filter)\n",
    "            \n",
    "            collocations = finder.score_ngrams(bigram_measures.pmi)\n",
    "            \n",
    "            collocate_stats = []\n",
    "            collocate_contexts = {}\n",
    "\n",
    "            for bigram, pmi in collocations:\n",
    "                if word_of_interest_lower in (word.lower() for word in bigram):\n",
    "                    other_word = bigram[0] if bigram[1].lower() == word_of_interest_lower else bigram[1]\n",
    "                    observed_freq = finder.ngram_fd[bigram] #observed frequency of bigram\n",
    "                    word_freq = finder.word_fd[other_word] #frequency of the other word\n",
    "                    expected_freq = (sum(finder.word_fd[word] for word in finder.word_fd if word.lower() == word_of_interest_lower) * word_freq) / total_tokens\n",
    "                    #Calculates the expected frequency of the bigram based on the frequencies of the individual words\n",
    "                    num_texts = sum(1 for text in window_texts if other_word.lower() in text.lower().split()) #how many texts the other word is in\n",
    "                    \n",
    "                    collocate_stats.append({\n",
    "                        'word': other_word,\n",
    "                        'total_corpus': word_freq,\n",
    "                        'expected_freq': expected_freq,\n",
    "                        'observed_freq': observed_freq,\n",
    "                        'num_texts': num_texts,\n",
    "                        'pmi': pmi\n",
    "                    })\n",
    "                    \n",
    "                    collocate_contexts[other_word] = []\n",
    "                    for text in window_texts:\n",
    "                        words = text.split()\n",
    "                        for i, word in enumerate(words):\n",
    "                            if word.lower() == word_of_interest_lower and other_word.lower() in [w.lower() for w in words[max(0, i-collocate_window):i+collocate_window+1]]:\n",
    "                                context = ' '.join(words[max(0, i-collocate_window):i+collocate_window+1])\n",
    "                                collocate_contexts[other_word].append((f\"Years {window_start}-{window_end}: {context}\", other_word))\n",
    "            \n",
    "            collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "            all_collocates = collocate_stats\n",
    "            top_collocates = collocate_stats[:top_n]\n",
    "            \n",
    "            filtered_contexts = []\n",
    "            for collocate in top_collocates:\n",
    "                filtered_contexts.extend(collocate_contexts.get(collocate['word'], []))\n",
    "            \n",
    "            collocates_per_window[f\"{window_start}-{window_end}\"] = {\n",
    "                'collocates': top_collocates,\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_texts': total_texts,\n",
    "                'contexts': filtered_contexts\n",
    "            }\n",
    "            print(f\"Years {window_start}-{window_end}: {len(all_collocates)} collocates found\")\n",
    "\n",
    "        return collocates_per_window\n",
    "\n",
    "    print(\"Starting text processing...\")\n",
    "    texts_by_year = read_texts_by_year(text_dirs)\n",
    "    collocates_by_window = get_collocates_by_sliding_window(texts_by_year)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving results to CSV files...\")\n",
    "\n",
    "    for window, data in collocates_by_window.items():\n",
    "        filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                             'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "            \n",
    "            for i, collocate in enumerate(data['collocates'], 1):\n",
    "                writer.writerow([\n",
    "                    i,\n",
    "                    collocate['word'],\n",
    "                    collocate['total_corpus'],\n",
    "                    f\"{collocate['expected_freq']:.2f}\",\n",
    "                    collocate['observed_freq'],\n",
    "                    collocate['num_texts'],\n",
    "                    f\"{collocate['pmi']:.2f}\"\n",
    "                ])\n",
    "\n",
    "        context_file = os.path.join(output_dir, f'collocate_contexts_{window}.csv')\n",
    "        with open(context_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Context', 'Collocate'])\n",
    "            for context, collocate in data['contexts']:\n",
    "                writer.writerow([context, collocate])\n",
    "\n",
    "    print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "    print(f\"Contexts saved to separate CSV files for each window in the '{output_dir}' directory.\")\n",
    "\n",
    "analyze_collocates([r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\", r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstl\", r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rsta\"], \"collocate_results_combined_ff5_old_ver\", \"fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_collocates(text_dirs, output_dir, word_of_interest, window_size=5, collocate_window=5, freq_filter=5, top_n=10):\n",
    "    def extract_year_from_filename(filename):\n",
    "        match = re.search(r'rst[bl]?_?(\\d{4})', filename)\n",
    "        return int(match.group(1)) if match else None\n",
    "\n",
    "    def read_texts_by_year(directories):\n",
    "        texts_by_year = defaultdict(list)\n",
    "        print(\"Reading text files...\")\n",
    "        for directory in directories:\n",
    "            for filename in os.listdir(directory):\n",
    "                if filename.endswith('.txt'):\n",
    "                    year = extract_year_from_filename(filename)\n",
    "                    if year:\n",
    "                        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                            text = file.read()\n",
    "                        text = text.replace('ſ', 's').replace('Å¿', 's').replace('obseryed', 'observed')\n",
    "                        texts_by_year[year].append(text)\n",
    "        print(\"Text files read and organized by year.\")\n",
    "        return texts_by_year\n",
    "\n",
    "    stop_words = set(stopwords.words('english')).union({'al', 'thab', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'})\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def process_tokens(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens \n",
    "                if token.isalpha() and len(token) > 1 and token not in stop_words and token not in string.punctuation]\n",
    "\n",
    "    def get_collocates_by_sliding_window(texts_by_year):\n",
    "        collocates_per_window = {}\n",
    "        all_years = sorted(texts_by_year.keys())\n",
    "        start_year, end_year = min(all_years), max(all_years)\n",
    "\n",
    "        print(\"Analyzing collocates by sliding window...\")\n",
    "        for window_start in range(start_year, end_year - window_size + 2):\n",
    "            window_end = window_start + window_size - 1\n",
    "            window_texts = [text for year in range(window_start, window_end + 1) \n",
    "                            for text in texts_by_year.get(year, [])]\n",
    "            \n",
    "            all_tokens = [token for text in window_texts for token in process_tokens(text)]\n",
    "            \n",
    "            total_tokens = len(all_tokens)\n",
    "            total_texts = len(window_texts)\n",
    "            \n",
    "            finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "            finder.apply_freq_filter(freq_filter)\n",
    "            \n",
    "            collocations = finder.score_ngrams(BigramAssocMeasures().pmi)\n",
    "            \n",
    "            collocate_stats = []\n",
    "            collocate_contexts = defaultdict(list)\n",
    "\n",
    "            word_of_interest_lower = word_of_interest.lower()\n",
    "            word_of_interest_freq = sum(finder.word_fd[word] for word in finder.word_fd if word.lower() == word_of_interest_lower)\n",
    "\n",
    "            for bigram, pmi in collocations:\n",
    "                if word_of_interest_lower in (word.lower() for word in bigram):\n",
    "                    other_word = bigram[0] if bigram[1].lower() == word_of_interest_lower else bigram[1]\n",
    "                    observed_freq = finder.ngram_fd[bigram]\n",
    "                    word_freq = finder.word_fd[other_word]\n",
    "                    expected_freq = (word_of_interest_freq * word_freq) / total_tokens\n",
    "                    num_texts = sum(1 for text in window_texts if other_word.lower() in text.lower().split())\n",
    "                    \n",
    "                    collocate_stats.append({\n",
    "                        'word': other_word,\n",
    "                        'total_corpus': word_freq,\n",
    "                        'expected_freq': expected_freq,\n",
    "                        'observed_freq': observed_freq,\n",
    "                        'num_texts': num_texts,\n",
    "                        'pmi': pmi\n",
    "                    })\n",
    "                    \n",
    "                    for text in window_texts:\n",
    "                        words = text.split()\n",
    "                        for i, word in enumerate(words):\n",
    "                            if word.lower() == word_of_interest_lower and other_word.lower() in [w.lower() for w in words[max(0, i-collocate_window):i+collocate_window+1]]:\n",
    "                                context = ' '.join(words[max(0, i-collocate_window):i+collocate_window+1])\n",
    "                                collocate_contexts[other_word].append((f\"Years {window_start}-{window_end}: {context}\", other_word))\n",
    "            \n",
    "            collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "            top_collocates = collocate_stats[:top_n]\n",
    "            \n",
    "            filtered_contexts = [context for collocate in top_collocates for context in collocate_contexts.get(collocate['word'], [])]\n",
    "            \n",
    "            collocates_per_window[f\"{window_start}-{window_end}\"] = {\n",
    "                'collocates': top_collocates,\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_texts': total_texts,\n",
    "                'contexts': filtered_contexts\n",
    "            }\n",
    "            print(f\"Years {window_start}-{window_end}: {len(collocate_stats)} collocates found\")\n",
    "\n",
    "        return collocates_per_window\n",
    "\n",
    "    print(\"Starting text processing...\")\n",
    "    texts_by_year = read_texts_by_year(text_dirs)\n",
    "    collocates_by_window = get_collocates_by_sliding_window(texts_by_year)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving results to CSV files...\")\n",
    "\n",
    "    for window, data in collocates_by_window.items():\n",
    "        filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                             'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "            \n",
    "            for i, collocate in enumerate(data['collocates'], 1):\n",
    "                writer.writerow([\n",
    "                    i,\n",
    "                    collocate['word'],\n",
    "                    collocate['total_corpus'],\n",
    "                    f\"{collocate['expected_freq']:.2f}\",\n",
    "                    collocate['observed_freq'],\n",
    "                    collocate['num_texts'],\n",
    "                    f\"{collocate['pmi']:.2f}\"\n",
    "                ])\n",
    "\n",
    "        context_file = os.path.join(output_dir, f'collocate_contexts_{window}.csv')\n",
    "        with open(context_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Context', 'Collocate'])\n",
    "            writer.writerows(data['contexts'])\n",
    "\n",
    "    print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "\n",
    "analyze_collocates([r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\", \n",
    "                    r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstl\", \n",
    "                    r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rsta\"], \n",
    "                   \"collocate_results_combined_fact\", \"fact\")\n",
    "\n",
    "analyze_collocates([r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\", \n",
    "                    r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstl\", \n",
    "                    r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rsta\"], \n",
    "                   \"collocate_results_combined_fiction\", \"fiction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
