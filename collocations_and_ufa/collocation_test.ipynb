{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix:\n",
    "- make sure the code isnt bullshit (since chatgpt etc)\n",
    "- maybe use a better measure than pmi? (as in the original paper, yet example uses pmi for ufa)\n",
    "- consider combining the rstb and rstl datasets (do many versions? one with all, 3 separate and one with rstl and rstb combined?)\n",
    "- maybe make the 5L 5R account for punctuation? since now it takes from even the preceding sentence (check if this is important)\n",
    "\n",
    "\n",
    "R error: couldn't execute b<-matrix(cli, nrow=0,ncol=6, byrow = TRUE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING CONTEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matter' 'known' 'many' 'important' 'respecting' 'new' 'following'\n",
      " 'state' 'case' 'however' 'prove' 'stated' 'experiment' 'preceding'\n",
      " 'ascertained' 'appear' 'general' 'may' 'adduced' 'chemical' 'series'\n",
      " 'number' 'observation' 'curious' 'observed' 'well' 'appears' 'singular'\n",
      " 'importance' 'propriety' 'certainty' 'noticed' 'principal' 'connected'\n",
      " 'interesting' 'proved' 'remarkable' 'point' 'might' 'would' 'whole'\n",
      " 'first' 'class' 'induction' 'accordance' 'recorded' 'knowledge'\n",
      " 'referred' 'experimental' 'shown' 'theory' 'made' 'found' 'additional'\n",
      " 'mentioned' 'foregoing' 'acid' 'striking' 'latter' 'fig' 'induced'\n",
      " 'already' 'described' 'explanation' 'similar' 'also' 'establish' 'far'\n",
      " 'substance' 'penetration' 'supported' 'mention' 'pointed' 'existence'\n",
      " 'think' 'animal' 'spermatozoon' 'long' 'cell' 'express' 'time'\n",
      " 'established' 'always' 'system' 'equation' 'attention' 'mere' 'quantic'\n",
      " 'equal' 'function' 'form' 'seems' 'regard' 'account' 'order' 'body'\n",
      " 'line' 'upon' 'brought' 'consider' 'evidence' 'tion' 'take' 'value'\n",
      " 'great' 'founded' 'must' 'air' 'tube' 'drawn' 'yet' 'together'\n",
      " 'therefore' 'evident' 'probable' 'explain' 'could' 'fibre' 'surface'\n",
      " 'due' 'curve' 'anatomical' 'expression' 'arises' 'conic' 'owing'\n",
      " 'specimen' 'bearing' 'explained' 'relation' 'another' 'given' 'interest'\n",
      " 'partly' 'noted' 'statement' 'question' 'even' 'obtained' 'confirmed'\n",
      " 'certain' 'difference' 'detailed' 'interpretation' 'mind' 'view' 'much'\n",
      " 'show' 'tends' 'main' 'becomes' 'consistent' 'relating' 'teleostei'\n",
      " 'merely' 'considerable' 'almost' 'indicated' 'polyp' 'doubt' 'nerve'\n",
      " 'find' 'taken' 'change' 'seen' 'less' 'tt' 'accounted' 'film'\n",
      " 'discovered' 'broad' 'lie' 'arrange' 'fundamental' 'beyond' 'since'\n",
      " 'notwithstanding' 'spite' 'came' 'suggested' 'previously' 'seem' 'shall'\n",
      " 'group' 'gas' 'significant' 'demonstrated' 'taking' 'remains' 'present'\n",
      " 'condition' 'thus' 'result' 'consequence' 'represent' 'consideration'\n",
      " 'bundle' 'electric' 'although' 'still' 'light' 'temperature' 'familiar'\n",
      " 'often' 'specie' 'normal' 'large' 'justified' 'led' 'note' 'distribution'\n",
      " 'follows' 'though' 'septum' 'give' 'clearly' 'clear' 'character'\n",
      " 'without' 'accord' 'depends' 'type' 'place' 'struck' 'indicate' 'come'\n",
      " 'single' 'plant' 'velocity' 'actual' 'lead' 'constant' 'agreement' 'leaf'\n",
      " 'u' 'effect' 'pressure' 'wire' 'young' 'low' 'maximum' 'part' 'expected'\n",
      " 'negative' 'possible' 'hydrogen' 'water' 'second' 'small' 'extremely'\n",
      " 'individual' 'method' 'despite' 'mean' 'set' 'reveals' 'emphasise'\n",
      " 'colour' 'scale' 'somewhat' 'apart' 'injection' 'using' 'position' 'none'\n",
      " 'suggest' 'correlated' 'ridge' 'development' 'regarding' 'corresponding'\n",
      " 'solution' 'recognition' 'complicated' 'primary' 'see' 'sulcus' 'region'\n",
      " 'make' 'emerges' 'chromosome' 'figure' 'borne' 'exist' 'stele' 'larva'\n",
      " 'concerning' 'arise' 'several' 'initial' 'used' 'suggests' 'provided'\n",
      " 'usually' 'use' 'record' 'differ' 'occur' 'reference' 'stylized'\n",
      " 'reflects' 'reflecting' 'empirical' 'compounded' 'reflect' 'sheet'\n",
      " 'basic' 'different' 'study' 'model' 'fiction' 'motivated' 'exploit'\n",
      " 'highlighted' 'highlight' 'advantage' 'nonlinear' 'stem' 'based'\n",
      " 'genetic' 'component' 'natural' 'evolution' 'brain' 'derives' 'relies'\n",
      " 'emphasize' 'refers' 'reflected' 'opinion' 'quite' 'illustrated'\n",
      " 'majority' 'true' 'equivalent' 'related' 'scientific' 'called' 'simple'\n",
      " 'finding' 'hypothesis' 'cause' 'highly' 'previous' 'recently'\n",
      " 'uncertainty' 'multiple' 'power' 'task' 'key' 'among' 'infection' 'way'\n",
      " 'table' 'term' 'size' 'neuron' 'approach' 'pattern' 'energy' 'process'\n",
      " 'example' 'gene' 'human' 'data' 'virtue' 'attributable' 'aware'\n",
      " 'exploited' 'attributed' 'obvious' 'implies' 'heating' 'widely'\n",
      " 'considering' 'every' 'simply' 'combined' 'course' 'definition'\n",
      " 'organism' 'probably' 'necessary' 'discussed' 'coupled' 'people' 'prior'\n",
      " 'patient' 'good' 'complexity' 'bird' 'defined' 'involved' 'object'\n",
      " 'performance' 'least' 'addition' 'along' 'shape' 'technique' 'quantum'\n",
      " 'tree' 'whether' 'range' 'complex' 'specific' 'property' 'social'\n",
      " 'measurement' 'early' 'interaction' 'issue' 'field' 'signal'\n",
      " 'information' 'impact' 'high' 'population' 'mechanism' 'control'\n",
      " 'analysis' 'structure' 'activity' 'material' 'trier' 'hampered' 'relates'\n",
      " 'exploiting' 'chaperone' 'surprising' 'device' 'telomerase' 'except'\n",
      " 'proof' 'nevertheless' 'infant' 'concern' 'compound' 'real' 'primate'\n",
      " 'situation' 'caused' 'limited' 'classical' 'strongly' 'combination'\n",
      " 'relevant' 'indeed' 'little' 'conclusion' 'numerical' 'according'\n",
      " 'theoretical' 'total' 'scenario' 'world' 'recent' 'others' 'framework'\n",
      " 'error' 'whereas' 'ability' 'technology' 'local' 'cost' 'rather'\n",
      " 'particle' 'event' 'problem' 'need' 'mode' 'life' 'simulation' 'space'\n",
      " 'factor' 'selection' 'dna' 'including' 'sample' 'increase' 'disease'\n",
      " 'level' 'role' 'dynamic' 'response' 'area' 'arithmetical' 'exacerbated'\n",
      " 'arithmetic' 'evidenced' 'subduction' 'rely' 'follow' 'rest' 'understood'\n",
      " 'abstract' 'turn' 'capture' 'photon' 'lattice' 'success' 'assumption'\n",
      " 'description' 'require' 'phenomenon' 'zero' 'reason' 'difficult'\n",
      " 'telomere' 'dimension' 'iii' 'occurs' 'hand' 'capacity' 'profile'\n",
      " 'central' 'presence' 'correlation' 'able' 'section' 'directly' 'learning'\n",
      " 'respectively' 'language' 'child' 'representation' 'variable' 'linear'\n",
      " 'fact' 'context' 'stress' 'standard' 'larger' 'required' 'origin'\n",
      " 'particular' 'ii' 'transmission' 'period' 'wave' 'action' 'lower'\n",
      " 'likely' 'parameter' 'frequency' 'environment' 'higher' 'current'\n",
      " 'author' 'rate' 'finder' 'underscore' 'accounting' 'illustrates'\n",
      " 'infinite' 'easy' 'something' 'harmonic' 'unlike' 'illustrate' 'observe'\n",
      " 'half' 'crucial' 'know' 'allows' 'path' 'connection' 'moreover'\n",
      " 'typically' 'deep' 'act' 'full' 'participant' 'amplitude' 'experience'\n",
      " 'core' 'proposed' 'characteristic' 'positive' 'either' 'sex' 'boundary'\n",
      " 'length' 'measure' 'right' 'application' 'increased' 'flow' 'year'\n",
      " 'associated' 'protein' 'behaviour' 'science' 'translates' 'trivial'\n",
      " 'semigroup' 'remark' 'consists' 'eigenvalue' 'refer' 'picture' 'causal'\n",
      " 'sometimes' 'easily' 'apparent' 'physical' 'argument' 'category' 'define'\n",
      " 'characterized' 'mathematical' 'map' 'theorem' 'operator' 'useful'\n",
      " 'principle' 'close' 'limit' 'become' 'smaller' 'independent' 'word'\n",
      " 'relatively' 'family' 'considered' 'detail' 'behavioural' 'presented'\n",
      " 'generation' 'step' 'concept' 'strong' 'increasing' 'loss' 'direct'\n",
      " 'estimate' 'probability' 'measured' 'matrix' 'biological' 'subject'\n",
      " 'male' 'female' 'source' 'concentration' 'future' 'molecular' 'provide'\n",
      " 'global' 'site' 'nature' 'network' 'work' 'phase' 'research' 'undermined'\n",
      " 'fallacy' 'emphasizes' 'besides' 'axiom' 'contrary' 'imply' 'never'\n",
      " 'regardless' 'asymptotic' 'recognized' 'rich' 'largest' 'culture'\n",
      " 'underlying' 'ion' 'provides' 'effective' 'communication' 'ing'\n",
      " 'cultural' 'common' 'memory' 'density' 'support' 'trait' 'http'\n",
      " 'proposition' 'reflection' 'precisely' 'speaker' 'smooth' 'argue'\n",
      " 'isolation' 'divergence' 'monkey' 'emotion' 'reward' 'ground' 'weak'\n",
      " 'periodic' 'agent' 'chimpanzee' 'practice' 'obtain' 'pair' 'fusion'\n",
      " 'regime' 'law' 'side' 'composition' 'aspect' 'reduced' 'better' 'input'\n",
      " 'domain' 'element' 'distance' 'resource' 'risk' 'feature' 'community'\n",
      " 'potential' 'across' 'lsw' 'cosmology' 'former' 'created' 'expect'\n",
      " 'address' 'changing' 'last' 'shift' 'showed' nan]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Word'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[0;32m     16\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(f)\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWord\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[0;32m     18\u001b[0m         keyword_docs\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keyword_docs:\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Igiba\\.pyenv\\pyenv-win\\versions\\3.9.10\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Word'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"collocate_results_combined_fact_latest_slow\"\n",
    "\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "combined_data = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "unique_words = combined_data['Word'].unique()\n",
    "print(unique_words)\n",
    "\n",
    "keyword = \"trier\"\n",
    "keyword_docs = []\n",
    "\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if keyword in df['Word'].values:\n",
    "        keyword_docs.append(f)\n",
    "\n",
    "if keyword_docs:\n",
    "    print(f\"The keyword '{keyword}' is found in the following document(s):\")\n",
    "    for doc in keyword_docs:\n",
    "        print(doc)\n",
    "else:\n",
    "    print(f\"The keyword '{keyword}' is not found in any document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import traceback\n",
    "\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')).union({\"et\", 'al', \"tn\", 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'})\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r'rst[bla]?_?(\\d{4})', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        year = extract_year_from_filename(os.path.basename(file_path))\n",
    "        if not year:\n",
    "            return None\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        text = text.replace('ſ', 's').replace('Å¿', 's').replace('obseryed', 'observed').replace('thab', 'that')\n",
    "        return year, text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_tokens(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens \n",
    "            if token.isalpha() and len(token) > 1 and token not in stop_words and token not in string.punctuation]\n",
    "\n",
    "def process_window(window_data, word_of_interest, freq_filter, collocate_window): #add top_n at the end if using\n",
    "    try:\n",
    "        window_start, window_end, window_texts = window_data\n",
    "        all_tokens = [token for text in window_texts for token in process_tokens(text)]\n",
    "        \n",
    "        total_tokens = len(all_tokens)\n",
    "        total_texts = len(window_texts)\n",
    "        \n",
    "        finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "        finder.apply_freq_filter(freq_filter)\n",
    "        \n",
    "        collocations = finder.score_ngrams(BigramAssocMeasures().pmi)\n",
    "        \n",
    "        collocate_stats = []\n",
    "        collocate_contexts = defaultdict(list)\n",
    "\n",
    "        word_of_interest_lower = word_of_interest.lower()\n",
    "        word_of_interest_freq = sum(finder.word_fd[word] for word in finder.word_fd if word.lower() == word_of_interest_lower)\n",
    "\n",
    "        for bigram, pmi in collocations:\n",
    "            if word_of_interest_lower in (word.lower() for word in bigram):\n",
    "                other_word = bigram[0] if bigram[1].lower() == word_of_interest_lower else bigram[1]\n",
    "                observed_freq = finder.ngram_fd[bigram]\n",
    "                word_freq = finder.word_fd[other_word]\n",
    "                expected_freq = (word_of_interest_freq * word_freq) / total_tokens\n",
    "                num_texts = sum(1 for text in window_texts if other_word.lower() in text.lower().split())\n",
    "                \n",
    "                if num_texts > 1:\n",
    "                    collocate_stats.append({\n",
    "                        'word': other_word,\n",
    "                        'total_corpus': word_freq,\n",
    "                        'expected_freq': expected_freq,\n",
    "                        'observed_freq': observed_freq,\n",
    "                        'num_texts': num_texts,\n",
    "                        'pmi': pmi\n",
    "                    })\n",
    "                    \n",
    "                    for text in window_texts:\n",
    "                        words = text.split()\n",
    "                        for i, word in enumerate(words):\n",
    "                            if word.lower() == word_of_interest_lower and other_word.lower() in [w.lower() for w in words[max(0, i-collocate_window):i+collocate_window+1]]:\n",
    "                                context = ' '.join(words[max(0, i-collocate_window):i+collocate_window+1])\n",
    "                                collocate_contexts[other_word].append((f\"Years {window_start}-{window_end}: {context}\", other_word))\n",
    "        \n",
    "        collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "        top_collocates = collocate_stats#[:top_n] #take them all\n",
    "        \n",
    "        filtered_contexts = [context for collocate in top_collocates for context in collocate_contexts.get(collocate['word'], [])]\n",
    "        \n",
    "        return f\"{window_start}-{window_end}\", {\n",
    "            'collocates': top_collocates,\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_texts': total_texts,\n",
    "            'contexts': filtered_contexts\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing window {window_start}-{window_end}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text processing...\n",
      "Reading files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882bb1aa285e49668d0823f3aaf47499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing windows...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70b685e51f44ff2ad5e5fe5f63b9961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to CSV files...\n",
      "Collocate analysis completed. Results saved in the 'collocate_results_combined_fact_latest_slow' directory.\n"
     ]
    }
   ],
   "source": [
    "#slightly optimized version\n",
    "def analyze_collocates(text_dirs, output_dir, word_of_interest, window_size=5, collocate_window=5, freq_filter=5):\n",
    "    def read_texts_by_year(directories):\n",
    "        texts_by_year = defaultdict(list)\n",
    "        file_paths = [os.path.join(dir, f) for dir in directories for f in os.listdir(dir) if f.endswith('.txt')]\n",
    "        \n",
    "        print(\"Reading files...\")\n",
    "        for file_path in tqdm(file_paths):\n",
    "            result = process_file(file_path)\n",
    "            if result:\n",
    "                year, text = result\n",
    "                texts_by_year[year].append(text)\n",
    "        \n",
    "        return texts_by_year\n",
    "\n",
    "    print(\"Starting text processing...\")\n",
    "    texts_by_year = read_texts_by_year(text_dirs)\n",
    "    \n",
    "    all_years = sorted(texts_by_year.keys())\n",
    "    start_year, end_year = min(all_years), max(all_years)\n",
    "\n",
    "    window_data = [\n",
    "        (window_start, window_start + window_size - 1, \n",
    "         [text for year in range(window_start, window_start + window_size) for text in texts_by_year.get(year, [])])\n",
    "        for window_start in range(start_year, end_year - window_size + 2)\n",
    "    ]\n",
    "\n",
    "    collocates_by_window = {}\n",
    "    print(\"Processing windows...\")\n",
    "    for data in tqdm(window_data):\n",
    "        result = process_window(data, word_of_interest, freq_filter, collocate_window)\n",
    "        if result:\n",
    "            window, window_result = result\n",
    "            collocates_by_window[window] = window_result\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving results to CSV files...\")\n",
    "\n",
    "    for window, data in collocates_by_window.items():\n",
    "        filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                             'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "            \n",
    "            for i, collocate in enumerate(data['collocates'], 1):\n",
    "                writer.writerow([\n",
    "                    i,\n",
    "                    collocate['word'],\n",
    "                    collocate['total_corpus'],\n",
    "                    f\"{collocate['expected_freq']:.2f}\",\n",
    "                    collocate['observed_freq'],\n",
    "                    collocate['num_texts'],\n",
    "                    f\"{collocate['pmi']:.2f}\"\n",
    "                ])\n",
    "\n",
    "        context_file = os.path.join(output_dir, f'collocate_contexts_{window}.csv')\n",
    "        with open(context_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Context', 'Collocate'])\n",
    "            writer.writerows(data['contexts'])\n",
    "\n",
    "    print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "\n",
    "try:\n",
    "    analyze_collocates([r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\", \n",
    "                        r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstl\", \n",
    "                       r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rsta\"], \n",
    "                      \"collocate_results_combined_fact_latest_slow\", \"fact\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during analysis: {str(e)}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text processing...\n",
      "Reading files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2885cf4f8a04507b966a33f5d08f93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#txt file creating version\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import traceback\n",
    "\n",
    "\n",
    "def analyze_collocates(text_dirs, output_dir, word_of_interest, window_size=5, collocate_window=5, freq_filter=5):\n",
    "    def read_texts_by_year(directories):\n",
    "        texts_by_year = defaultdict(list)\n",
    "        file_paths = [os.path.join(dir, f) for dir in directories for f in os.listdir(dir) if f.endswith('.txt')]\n",
    "        \n",
    "        print(\"Reading files...\")\n",
    "        for file_path in tqdm(file_paths):\n",
    "            result = process_file(file_path)\n",
    "            if result:\n",
    "                year, text = result\n",
    "                texts_by_year[year].append(text)\n",
    "        \n",
    "        return texts_by_year\n",
    "\n",
    "    print(\"Starting text processing...\")\n",
    "    texts_by_year = read_texts_by_year(text_dirs)\n",
    "    \n",
    "    all_years = sorted(texts_by_year.keys())\n",
    "    start_year, end_year = min(all_years), max(all_years)\n",
    "\n",
    "    window_data = [\n",
    "        (window_start, window_start + window_size - 1, \n",
    "         [text for year in range(window_start, window_start + window_size) for text in texts_by_year.get(year, [])])\n",
    "        for window_start in range(start_year, end_year - window_size + 2)\n",
    "    ]\n",
    "\n",
    "    collocates_by_window = {}\n",
    "    print(\"Processing windows...\")\n",
    "    for data in tqdm(window_data):\n",
    "        result = process_window(data, word_of_interest, freq_filter, collocate_window)\n",
    "        if result:\n",
    "            window, window_result = result\n",
    "            collocates_by_window[window] = window_result\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving results to CSV and TXT files...\")\n",
    "\n",
    "    for window, data in collocates_by_window.items():\n",
    "        # Save CSV file\n",
    "        csv_filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                             'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "            \n",
    "            for i, collocate in enumerate(data['collocates'], 1):\n",
    "                writer.writerow([\n",
    "                    i,\n",
    "                    collocate['word'],\n",
    "                    collocate['total_corpus'],\n",
    "                    f\"{collocate['expected_freq']:.2f}\",\n",
    "                    collocate['observed_freq'],\n",
    "                    collocate['num_texts'],\n",
    "                    f\"{collocate['pmi']:.2f}\"\n",
    "                ])\n",
    "\n",
    "        # Save TXT file\n",
    "        txt_filename = os.path.join(output_dir, f'collocates_txt_{window}.txt')\n",
    "        with open(txt_filename, 'w', encoding='utf-8') as txtfile:\n",
    "            # Write fake header\n",
    "            txtfile.write(\"https://example.com/collocation-analysis\\n\")\n",
    "            txtfile.write(f\"There are {data['total_tokens']} different words in your collocation database for \\\"{word_of_interest}\\\". \")\n",
    "            txtfile.write(f\"(Your query returned {sum(c['observed_freq'] for c in data['collocates'])} matches in {data['total_texts']} different texts)\\n\")\n",
    "            txtfile.write(\"__________________\\n\\n\")\n",
    "            \n",
    "            # Write column headers\n",
    "            txtfile.write(\"No.\\tWord\\tTotal no. in whole corpus\\tExpected collocate frequency\\t\")\n",
    "            txtfile.write(\"Observed collocate frequency\\tIn no. of texts\\tMutual information value\\n\\n\")\n",
    "            \n",
    "            # Write data rows\n",
    "            for i, collocate in enumerate(data['collocates'], 1):\n",
    "                txtfile.write(f\"{i}\\t{collocate['word']}\\t{collocate['total_corpus']}\\t\")\n",
    "                txtfile.write(f\"{collocate['expected_freq']:.2f}\\t{collocate['observed_freq']}\\t\")\n",
    "                txtfile.write(f\"{collocate['num_texts']}\\t{collocate['pmi']:.2f}\\n\")\n",
    "\n",
    "        # Save context file (CSV)\n",
    "        context_file = os.path.join(output_dir, f'collocate_contexts_{window}.csv')\n",
    "        with open(context_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Context', 'Collocate'])\n",
    "            writer.writerows(data['contexts'])\n",
    "\n",
    "    print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "\n",
    "# Usage remains the same:\n",
    "analyze_collocates([r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\", \n",
    "                     r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstl\", \n",
    "                     r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rsta\"], \n",
    "                   \"collocate_results_combined_fact_new_ver\", \"fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed. Results saved in the 'collocate_results_txt' directory.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def convert_csv_to_txt(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.startswith('collocates_') and filename.endswith('.csv'):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            output_filename = filename.replace('collocates_', 'collocates_txt_').replace('.csv', '.txt')\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            with open(input_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                data = list(reader)\n",
    "            \n",
    "            total_words = sum(int(row['Total no. in window corpus']) for row in data)\n",
    "            num_matches = sum(int(row['Observed collocate frequency']) for row in data)\n",
    "            num_texts = len(set(row['In no. of texts'] for row in data))\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as txtfile:\n",
    "                # Write fake header\n",
    "                txtfile.write(\"https://example.com/collocation-analysis\\n\")\n",
    "                txtfile.write(f\"There are {total_words} different words in your collocation database. \")\n",
    "                txtfile.write(f\"(Your query returned {num_matches} matches in {num_texts} different texts)\\n\")\n",
    "                txtfile.write(\"__________________\\n\\n\")\n",
    "                \n",
    "                # Write column headers\n",
    "                txtfile.write(\"No.\\tWord\\tTotal no. in whole corpus\\tExpected collocate frequency\\t\")\n",
    "                txtfile.write(\"Observed collocate frequency\\tIn no. of texts\\tMutual information value\\n\\n\")\n",
    "                \n",
    "                # Write data rows\n",
    "                for i, row in enumerate(data, 1):\n",
    "                    txtfile.write(f\"{i}\\t{row['Word']}\\t{row['Total no. in window corpus']}\\t\")\n",
    "                    txtfile.write(f\"{row['Expected collocate frequency']}\\t{row['Observed collocate frequency']}\\t\")\n",
    "                    txtfile.write(f\"{row['In no. of texts']}\\t{row['Mutual Information value']}\\n\")\n",
    "    \n",
    "    print(f\"Conversion completed. Results saved in the '{output_dir}' directory.\")\n",
    "\n",
    "# Usage example:\n",
    "convert_csv_to_txt('collocate_results_combined_fact_latest_slow', 'collocate_results_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "time.sleep(100)\n",
    "os.system(\"shutdown /s /t 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected delimiter: ,\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_file_delimiter(file, check_lines=2):\n",
    "    delimiters = [',', '\\t', ';', '|', ':']\n",
    "    results = defaultdict(int)\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i > check_lines:\n",
    "                break\n",
    "            for delimiter in delimiters:\n",
    "                # Use re.split to split based on the delimiter\n",
    "                fields = re.split(re.escape(delimiter), line)\n",
    "                if len(fields) > 1:\n",
    "                    results[delimiter] += 1\n",
    "\n",
    "    # Get the delimiter with the maximum count\n",
    "    best_delimiter = max(results, key=results.get)\n",
    "    return best_delimiter\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'collocate_results_combined_fact2/collocates_2015-2019.csv'\n",
    "delimiter = get_file_delimiter(file_path)\n",
    "print(f\"Detected delimiter: {delimiter}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
