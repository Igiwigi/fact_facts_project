{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix:\n",
    "- make sure the code isnt bullshit (since chatgpt etc)\n",
    "- maybe use a better measure than pmi?\n",
    "- make it a more concise set of functions\n",
    "- find out the contexts\n",
    "- consider combining the rstb and rstl datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING CONTEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text processing...\n",
      "Reading text files...\n",
      "Text files read and organized by year.\n",
      "Analyzing collocates by sliding window...\n",
      "Years 1887-1891: 243 collocates found\n",
      "Years 1888-1892: 214 collocates found\n",
      "Years 1889-1893: 237 collocates found\n",
      "Years 1890-1894: 320 collocates found\n",
      "Years 1891-1895: 359 collocates found\n",
      "Years 1892-1896: 337 collocates found\n",
      "Years 1893-1897: 357 collocates found\n",
      "Years 1894-1898: 352 collocates found\n",
      "Years 1895-1899: 264 collocates found\n",
      "Years 1896-1900: 258 collocates found\n",
      "Years 1897-1901: 253 collocates found\n",
      "Years 1898-1902: 180 collocates found\n",
      "Years 1899-1903: 145 collocates found\n",
      "Years 1900-1904: 189 collocates found\n",
      "Years 1901-1905: 158 collocates found\n",
      "Years 1902-1906: 181 collocates found\n",
      "Years 1903-1907: 181 collocates found\n",
      "Years 1904-1908: 183 collocates found\n",
      "Years 1905-1909: 190 collocates found\n",
      "Years 1906-1910: 157 collocates found\n",
      "Years 1907-1911: 124 collocates found\n",
      "Years 1908-1912: 160 collocates found\n",
      "Years 1909-1913: 138 collocates found\n",
      "Years 1910-1914: 160 collocates found\n",
      "Years 1911-1915: 196 collocates found\n",
      "Years 1912-1916: 203 collocates found\n",
      "Years 1913-1917: 170 collocates found\n",
      "Years 1914-1918: 172 collocates found\n",
      "Years 1915-1919: 71 collocates found\n",
      "Years 1916-1920: 83 collocates found\n",
      "Years 1917-1921: 90 collocates found\n",
      "Years 1918-1922: 90 collocates found\n",
      "Years 1919-1923: 103 collocates found\n",
      "Years 1920-1924: 123 collocates found\n",
      "Years 1921-1925: 121 collocates found\n",
      "Years 1922-1926: 107 collocates found\n",
      "Years 1923-1927: 150 collocates found\n",
      "Years 1924-1928: 145 collocates found\n",
      "Years 1925-1929: 170 collocates found\n",
      "Years 1926-1930: 164 collocates found\n",
      "Years 1927-1931: 214 collocates found\n",
      "Years 1928-1932: 257 collocates found\n",
      "Years 1929-1933: 227 collocates found\n",
      "Years 1930-1934: 242 collocates found\n",
      "Years 1931-1935: 244 collocates found\n",
      "Years 1932-1936: 193 collocates found\n",
      "Years 1933-1937: 208 collocates found\n",
      "Years 1934-1938: 244 collocates found\n",
      "Years 1935-1939: 196 collocates found\n",
      "Years 1936-1940: 174 collocates found\n",
      "Years 1937-1941: 145 collocates found\n",
      "Years 1938-1942: 80 collocates found\n",
      "Years 1939-1943: 52 collocates found\n",
      "Years 1940-1944: 28 collocates found\n",
      "Years 1941-1945: 23 collocates found\n",
      "Years 1942-1946: 26 collocates found\n",
      "Years 1943-1947: 62 collocates found\n",
      "Years 1944-1948: 75 collocates found\n",
      "Years 1945-1949: 89 collocates found\n",
      "Years 1946-1950: 125 collocates found\n",
      "Years 1947-1951: 165 collocates found\n",
      "Years 1948-1952: 139 collocates found\n",
      "Years 1949-1953: 158 collocates found\n",
      "Years 1950-1954: 144 collocates found\n",
      "Years 1951-1955: 107 collocates found\n",
      "Years 1952-1956: 57 collocates found\n",
      "Years 1953-1957: 27 collocates found\n",
      "Years 1954-1958: 2 collocates found\n",
      "Years 1955-1959: 0 collocates found\n",
      "Years 1956-1960: 0 collocates found\n",
      "Years 1957-1961: 0 collocates found\n",
      "Years 1958-1962: 0 collocates found\n",
      "Years 1959-1963: 0 collocates found\n",
      "Years 1960-1964: 0 collocates found\n",
      "Years 1961-1965: 0 collocates found\n",
      "Years 1962-1966: 0 collocates found\n",
      "Years 1963-1967: 0 collocates found\n",
      "Years 1964-1968: 0 collocates found\n",
      "Years 1965-1969: 0 collocates found\n",
      "Years 1966-1970: 0 collocates found\n",
      "Years 1967-1971: 0 collocates found\n",
      "Years 1968-1972: 0 collocates found\n",
      "Years 1969-1973: 0 collocates found\n",
      "Years 1970-1974: 0 collocates found\n",
      "Years 1971-1975: 0 collocates found\n",
      "Years 1972-1976: 0 collocates found\n",
      "Years 1973-1977: 0 collocates found\n",
      "Years 1974-1978: 0 collocates found\n",
      "Years 1975-1979: 0 collocates found\n",
      "Years 1976-1980: 0 collocates found\n",
      "Years 1977-1981: 0 collocates found\n",
      "Years 1978-1982: 0 collocates found\n",
      "Years 1979-1983: 0 collocates found\n",
      "Years 1980-1984: 0 collocates found\n",
      "Years 1981-1985: 0 collocates found\n",
      "Years 1982-1986: 0 collocates found\n",
      "Years 1983-1987: 0 collocates found\n",
      "Years 1984-1988: 0 collocates found\n",
      "Years 1985-1989: 0 collocates found\n",
      "Years 1986-1990: 0 collocates found\n",
      "Years 1987-1991: 0 collocates found\n",
      "Years 1988-1992: 0 collocates found\n",
      "Years 1989-1993: 0 collocates found\n",
      "Years 1990-1994: 0 collocates found\n",
      "Years 1991-1995: 0 collocates found\n",
      "Years 1992-1996: 0 collocates found\n",
      "Years 1993-1997: 0 collocates found\n",
      "Years 1994-1998: 0 collocates found\n",
      "Years 1995-1999: 0 collocates found\n",
      "Years 1996-2000: 1 collocates found\n",
      "Years 1997-2001: 2 collocates found\n",
      "Years 1998-2002: 2 collocates found\n",
      "Years 1999-2003: 2 collocates found\n",
      "Years 2000-2004: 2 collocates found\n",
      "Years 2001-2005: 1 collocates found\n",
      "Years 2002-2006: 5 collocates found\n",
      "Years 2003-2007: 10 collocates found\n",
      "Years 2004-2008: 29 collocates found\n",
      "Years 2005-2009: 59 collocates found\n",
      "Years 2006-2010: 103 collocates found\n",
      "Years 2007-2011: 117 collocates found\n",
      "Years 2008-2012: 157 collocates found\n",
      "Years 2009-2013: 288 collocates found\n",
      "Years 2010-2014: 431 collocates found\n",
      "Years 2011-2015: 627 collocates found\n",
      "Years 2012-2016: 816 collocates found\n",
      "Years 2013-2017: 984 collocates found\n",
      "Years 2014-2018: 948 collocates found\n",
      "Years 2015-2019: 1087 collocates found\n",
      "Years 2016-2020: 1082 collocates found\n",
      "Years 2017-2021: 1089 collocates found\n",
      "Years 2018-2022: 1002 collocates found\n",
      "Years 2019-2023: 896 collocates found\n",
      "Years 2020-2024: 542 collocates found\n",
      "Saving results to CSV files...\n",
      "Collocate analysis completed. Results saved in the 'collocate_results_rstb' directory.\n",
      "Contexts saved to separate files for each window in the 'collocate_results_rstb' directory.\n"
     ]
    }
   ],
   "source": [
    "#1st version, no context\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "text_dir = r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\"\n",
    "\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r'rst[bl]?_(\\d{4})', filename)\n",
    "    if not match:\n",
    "        match = re.search(r'rst[bl]?(\\d{4})', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def read_texts_by_year(directory):\n",
    "    texts_by_year = defaultdict(list)\n",
    "    print(\"Reading text files...\")\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            year = extract_year_from_filename(filename)\n",
    "            if year:\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                texts_by_year[year].append(text)\n",
    "    print(\"Text files read and organized by year.\")\n",
    "    return texts_by_year\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "def get_collocates_by_sliding_window(texts_by_year, word_of_interest, window_size=5):\n",
    "    collocates_per_window = {}\n",
    "    all_years = sorted(texts_by_year.keys())\n",
    "    start_year = min(all_years)\n",
    "    end_year = max(all_years)\n",
    "\n",
    "    print(\"Analyzing collocates by sliding window...\")\n",
    "    for window_start in range(start_year, end_year - window_size + 2):\n",
    "        window_end = window_start + window_size - 1\n",
    "        window_texts = []\n",
    "        for year in range(window_start, window_end + 1):\n",
    "            if year in texts_by_year:\n",
    "                window_texts.extend(texts_by_year[year])\n",
    "        \n",
    "        all_tokens = []\n",
    "        for text in window_texts:\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [token.lower() for token in tokens if token not in string.punctuation and token.lower() not in stop_words and not token.isdigit()] #removing tokens already here (stopwords, punctuation, digits)\n",
    "            all_tokens.extend(tokens) \n",
    "        \n",
    "        tokens = lemmatize_tokens(all_tokens) #should allow for seeing different forms of fact, but potentially bad to lemmatize?\n",
    "        \n",
    "        total_tokens = len(tokens) #total tokens dont include removed ones; is this bad?\n",
    "        total_texts = len(window_texts)\n",
    "        \n",
    "        bigram_measures = BigramAssocMeasures()\n",
    "        finder = BigramCollocationFinder.from_words(tokens)\n",
    "        finder.apply_freq_filter(2) #maybe make it higher to include less (2 is quite low)\n",
    "        \n",
    "        collocations = finder.score_ngrams(bigram_measures.pmi)\n",
    "        \n",
    "        collocate_stats = []\n",
    "        collocate_contexts = []\n",
    "\n",
    "        for bigram, pmi in collocations:\n",
    "            if word_of_interest in bigram:\n",
    "                other_word = bigram[0] if bigram[1] == word_of_interest else bigram[1] #dont fully understand this yet, are bigrams 5L 5R?\n",
    "                observed_freq = finder.ngram_fd[bigram]\n",
    "                word_freq = finder.word_fd[other_word]\n",
    "                expected_freq = (finder.word_fd[word_of_interest] * word_freq) / total_tokens\n",
    "                num_texts = sum(1 for text in window_texts if other_word in text.split())\n",
    "                \n",
    "                # Extract contexts with 5 words to the left and right  (BUT THE BIGRAM SHOULD BE 5L 5R)\n",
    "                for text in window_texts:\n",
    "                    words = text.split()\n",
    "                    for i, word in enumerate(words):\n",
    "                        if word == word_of_interest and other_word in words[max(0, i-5):i+6]:\n",
    "                            context = ' '.join(words[max(0, i-5):i+6])\n",
    "                            collocate_contexts.append(f\"Years {window_start}-{window_end}: {context}\")      \n",
    "\n",
    "                collocate_stats.append({\n",
    "                    'word': other_word,\n",
    "                    'total_corpus': word_freq,\n",
    "                    'expected_freq': expected_freq,\n",
    "                    'observed_freq': observed_freq,\n",
    "                    'num_texts': num_texts,\n",
    "                    'pmi': pmi\n",
    "                })\n",
    "        \n",
    "        collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "        collocates_per_window[f\"{window_start}-{window_end}\"] = {\n",
    "            'collocates': collocate_stats[:20],\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_texts': total_texts,\n",
    "            'contexts': collocate_contexts\n",
    "        }\n",
    "        print(f\"Years {window_start}-{window_end}: {len(collocate_stats)} collocates found\")\n",
    "\n",
    "    return collocates_per_window\n",
    "\n",
    "print(\"Starting text processing...\")\n",
    "texts_by_year = read_texts_by_year(text_dir)\n",
    "collocates_by_window = get_collocates_by_sliding_window(texts_by_year, \"fact\")\n",
    "\n",
    "# Saving collocation results to CSV files\n",
    "output_dir = 'collocate_results_rstb'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"Saving results to CSV files...\")\n",
    "\n",
    "for window, data in collocates_by_window.items():\n",
    "    filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                         'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "        \n",
    "        for i, collocate in enumerate(data['collocates'], 1):\n",
    "            writer.writerow([\n",
    "                i,\n",
    "                collocate['word'],\n",
    "                collocate['total_corpus'],\n",
    "                f\"{collocate['expected_freq']:.2f}\",\n",
    "                collocate['observed_freq'],\n",
    "                collocate['num_texts'],\n",
    "                f\"{collocate['pmi']:.2f}\"\n",
    "            ])\n",
    "        \n",
    "        # Add summary statistics at the end of each file\n",
    "        writer.writerow([])\n",
    "        writer.writerow(['Total tokens in window', data['total_tokens']])\n",
    "        writer.writerow(['Total texts in window', data['total_texts']])\n",
    "\n",
    "    # Saving collocate contexts to a text file for each window\n",
    "    context_file = os.path.join(output_dir, f'collocate_contexts_{window}.txt')\n",
    "    with open(context_file, 'w', encoding='utf-8') as f:\n",
    "        for context in data['contexts']:\n",
    "            f.write(context + '\\n')\n",
    "\n",
    "print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "print(f\"Contexts saved to separate files for each window in the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_collocates(text_dir, output_dir, word_of_interest, window_size=5, collocate_window=5, freq_filter=3, top_n = 10):\n",
    "    def extract_year_from_filename(filename):\n",
    "        match = re.search(r'rst[bl]?_(\\d{4})', filename)\n",
    "        if not match:\n",
    "            match = re.search(r'rst[bl]?(\\d{4})', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "\n",
    "    def read_texts_by_year(directory):\n",
    "        texts_by_year = defaultdict(list)\n",
    "        print(\"Reading text files...\")\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                year = extract_year_from_filename(filename)\n",
    "                if year:\n",
    "                    with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                        text = file.read()\n",
    "                    texts_by_year[year].append((filename, text))\n",
    "        print(\"Text files read and organized by year.\")\n",
    "        return texts_by_year\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_tokens(tokens):\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "    def get_collocates_by_sliding_window(texts_by_year):\n",
    "        collocates_per_window = {}\n",
    "        all_years = sorted(texts_by_year.keys())\n",
    "        start_year = min(all_years)\n",
    "        end_year = max(all_years)\n",
    "\n",
    "        print(\"Analyzing collocates by sliding window...\")\n",
    "        for window_start in range(start_year, end_year - window_size + 2):\n",
    "            window_end = window_start + window_size - 1\n",
    "            window_texts = []\n",
    "            for year in range(window_start, window_end + 1):\n",
    "                if year in texts_by_year:\n",
    "                    window_texts.extend(texts_by_year[year])\n",
    "            \n",
    "            all_tokens = []\n",
    "            full_tokens = []\n",
    "            for _, text in window_texts:\n",
    "                tokens = word_tokenize(text)\n",
    "                full_tokens.extend(tokens) #non-changed tokens (nothing is omitted)\n",
    "                tokens = [token.lower() for token in tokens if token not in string.punctuation and token.lower() not in stop_words and not token.isdigit()] \n",
    "                all_tokens.extend(tokens)\n",
    "            \n",
    "            tokens = lemmatize_tokens(all_tokens)\n",
    "\n",
    "            total_tokens = len(full_tokens)\n",
    "            total_texts = len(window_texts)\n",
    "            \n",
    "            def window_based_bigrams(tokens, window_size):\n",
    "                for i in range(len(tokens)):\n",
    "                    for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "                        if i != j:\n",
    "                            yield (tokens[i], tokens[j])\n",
    "\n",
    "            bigram_measures = BigramAssocMeasures()\n",
    "            finder = BigramCollocationFinder.from_words(window_based_bigrams(all_tokens, collocate_window))\n",
    "            finder.apply_freq_filter(freq_filter)\n",
    "            \n",
    "            collocations = finder.score_ngrams(bigram_measures.pmi)\n",
    "            \n",
    "            collocate_stats = []\n",
    "            collocate_contexts = defaultdict(list)\n",
    "\n",
    "            for bigram, pmi in collocations:\n",
    "                if word_of_interest.lower() in [b.lower() for b in bigram]:  # Case-insensitive check\n",
    "                    other_word = bigram[0] if bigram[1].lower() == word_of_interest.lower() else bigram[1]\n",
    "                    observed_freq = finder.ngram_fd[bigram]\n",
    "                    word_freq = finder.word_fd[other_word]\n",
    "                    expected_freq = (finder.word_fd[word_of_interest.lower()] * word_freq) / total_tokens\n",
    "                    num_texts = sum(1 for _, text in window_texts if other_word in text.split())\n",
    "                    \n",
    "                    # Extract contexts with collocate_window words to the left and right\n",
    "                    for filename, text in window_texts:\n",
    "                        words = word_tokenize(text)\n",
    "                        for i in range(len(words)):\n",
    "                            if words[i].lower() == word_of_interest.lower():\n",
    "                                start = max(0, i - collocate_window)\n",
    "                                end = min(len(words), i + collocate_window + 1)\n",
    "                                context = words[start:end]\n",
    "                                if other_word.lower() in [w.lower() for w in context]:\n",
    "                                    context_str = ' '.join(context)\n",
    "                                    collocate_contexts[filename].append({\n",
    "                                        'collocate': other_word,\n",
    "                                        'context': context_str\n",
    "                                    })\n",
    "\n",
    "                    collocate_stats.append({\n",
    "                        'word': other_word,\n",
    "                        'total_corpus': word_freq,\n",
    "                        'expected_freq': expected_freq,\n",
    "                        'observed_freq': observed_freq,\n",
    "                        'num_texts': num_texts,\n",
    "                        'pmi': pmi\n",
    "                    })\n",
    "            \n",
    "            collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "            top_collocates = collocate_stats[:top_n]  # Limit contexts to the top 10 collocates (or as specified)\n",
    "            collocates_per_window[f\"{window_start}-{window_end}\"] = {\n",
    "                'collocates': top_collocates,\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_texts': total_texts,\n",
    "                'contexts': {k: v for k, v in collocate_contexts.items() if any(c['collocate'] in [t['word'] for t in top_collocates] for c in v)}\n",
    "            }\n",
    "            print(f\"Years {window_start}-{window_end}: {len(collocate_stats)} collocates found\")\n",
    "\n",
    "        return collocates_per_window\n",
    "\n",
    "    texts_by_year = read_texts_by_year(text_dir)\n",
    "    collocates_by_window = get_collocates_by_sliding_window(texts_by_year)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving results to CSV files...\")\n",
    "\n",
    "    for window, data in collocates_by_window.items():\n",
    "        filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                             'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "            \n",
    "            for i, collocate in enumerate(data['collocates'], 1):\n",
    "                writer.writerow([\n",
    "                    i,\n",
    "                    collocate['word'],\n",
    "                    collocate['total_corpus'],\n",
    "                    f\"{collocate['expected_freq']:.2f}\",\n",
    "                    collocate['observed_freq'],\n",
    "                    collocate['num_texts'],\n",
    "                    f\"{collocate['pmi']:.2f}\"\n",
    "                ])\n",
    "    \n",
    "        contexts_dir = os.path.join(output_dir, f'contexts_{window}')\n",
    "        os.makedirs(contexts_dir, exist_ok=True)\n",
    "        \n",
    "        for filename, contexts in data['contexts'].items():\n",
    "            context_file = os.path.join(contexts_dir, f'contexts_{os.path.splitext(filename)[0]}.csv')\n",
    "            with open(context_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(['Filename', 'Collocate', 'Context'])\n",
    "                for context in contexts:\n",
    "                    writer.writerow([filename, context['collocate'], context['context']])\n",
    "\n",
    "    print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "    print(f\"Contexts saved to separate CSV files for each text in subdirectories of '{output_dir}'.\")\n",
    "\n",
    "analyze_collocates(r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\", \"collocate_results_rstb_latest_5L5R\", \"fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text files...\n",
      "Text files read and organized by year.\n",
      "Analyzing collocates by sliding window...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 171\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollocate analysis completed. Results saved in the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContexts saved to separate CSV files for each text in subdirectories of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 171\u001b[0m \u001b[43manalyze_collocates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFact_fiction_corpus\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mroyal society\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtxt_rstb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFact_fiction_corpus\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mroyal society\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtxt_rstl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollocate_results_combined\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfact\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 130\u001b[0m, in \u001b[0;36manalyze_collocates\u001b[1;34m(text_dirs, output_dir, word_of_interest, window_size, collocate_window, freq_filter, top_n)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collocates_per_window\n\u001b[0;32m    129\u001b[0m texts_by_year \u001b[38;5;241m=\u001b[39m read_texts_by_year(text_dirs)\n\u001b[1;32m--> 130\u001b[0m collocates_by_window \u001b[38;5;241m=\u001b[39m \u001b[43mget_collocates_by_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_by_year\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving results to CSV files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 87\u001b[0m, in \u001b[0;36manalyze_collocates.<locals>.get_collocates_by_sliding_window\u001b[1;34m(texts_by_year)\u001b[0m\n\u001b[0;32m     84\u001b[0m collocate_contexts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bigram, pmi \u001b[38;5;129;01min\u001b[39;00m collocations:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word_of_interest\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [b\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bigram]:\n\u001b[0;32m     88\u001b[0m         other_word \u001b[38;5;241m=\u001b[39m bigram[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m bigram[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m word_of_interest\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m bigram[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     89\u001b[0m         observed_freq \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mngram_fd[bigram]\n",
      "Cell \u001b[1;32mIn[7], line 87\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     84\u001b[0m collocate_contexts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bigram, pmi \u001b[38;5;129;01min\u001b[39;00m collocations:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word_of_interest\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bigram]:\n\u001b[0;32m     88\u001b[0m         other_word \u001b[38;5;241m=\u001b[39m bigram[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m bigram[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m word_of_interest\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m bigram[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     89\u001b[0m         observed_freq \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mngram_fd[bigram]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_collocates(text_dirs, output_dir, word_of_interest, window_size=5, collocate_window=5, freq_filter=3, top_n=10):\n",
    "    def extract_year_from_filename(filename):\n",
    "        match = re.search(r'rst[bl]?_(\\d{4})', filename)\n",
    "        if not match:\n",
    "            match = re.search(r'rst[bl]?(\\d{4})', filename)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "\n",
    "    def read_texts_by_year(directories):\n",
    "        texts_by_year = defaultdict(list)\n",
    "        print(\"Reading text files...\")\n",
    "        for directory in directories:\n",
    "            for filename in os.listdir(directory):\n",
    "                if filename.endswith('.txt'):\n",
    "                    year = extract_year_from_filename(filename)\n",
    "                    if year:\n",
    "                        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                            text = file.read()\n",
    "                        texts_by_year[year].append((filename, text))\n",
    "        print(\"Text files read and organized by year.\")\n",
    "        return texts_by_year\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_tokens(tokens):\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "    def get_collocates_by_sliding_window(texts_by_year):\n",
    "        collocates_per_window = {}\n",
    "        all_years = sorted(texts_by_year.keys())\n",
    "        start_year = min(all_years)\n",
    "        end_year = max(all_years)\n",
    "\n",
    "        print(\"Analyzing collocates by sliding window...\")\n",
    "        for window_start in range(start_year, end_year - window_size + 2):\n",
    "            window_end = window_start + window_size - 1\n",
    "            window_texts = []\n",
    "            for year in range(window_start, window_end + 1):\n",
    "                if year in texts_by_year:\n",
    "                    window_texts.extend(texts_by_year[year])\n",
    "            \n",
    "            all_tokens = []\n",
    "            full_tokens = []\n",
    "            for _, text in window_texts:\n",
    "                tokens = word_tokenize(text)\n",
    "                full_tokens.extend(tokens) \n",
    "                tokens = [token.lower() for token in tokens if token not in string.punctuation and token.lower() not in stop_words and not token.isdigit()] \n",
    "                all_tokens.extend(tokens)\n",
    "            \n",
    "            tokens = lemmatize_tokens(all_tokens)\n",
    "\n",
    "            total_tokens = len(full_tokens)\n",
    "            total_texts = len(window_texts)\n",
    "            \n",
    "            def window_based_bigrams(tokens, window_size):\n",
    "                for i in range(len(tokens)):\n",
    "                    for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "                        if i != j:\n",
    "                            yield (tokens[i], tokens[j])\n",
    "\n",
    "            bigram_measures = BigramAssocMeasures()\n",
    "            finder = BigramCollocationFinder.from_words(window_based_bigrams(tokens, collocate_window))\n",
    "            finder.apply_freq_filter(freq_filter)\n",
    "            \n",
    "            collocations = finder.score_ngrams(bigram_measures.pmi)\n",
    "            \n",
    "            collocate_stats = []\n",
    "            collocate_contexts = defaultdict(list)\n",
    "\n",
    "            for bigram, pmi in collocations:\n",
    "                if word_of_interest.lower() in [b.lower() for b in bigram]:  # Case-insensitive check\n",
    "                    other_word = bigram[0] if bigram[1].lower() == word_of_interest.lower() else bigram[1]\n",
    "                    observed_freq = finder.ngram_fd[bigram]\n",
    "                    word_freq = finder.word_fd[other_word]\n",
    "                    expected_freq = (finder.word_fd[word_of_interest.lower()] * word_freq) / total_tokens\n",
    "                    num_texts = sum(1 for _, text in window_texts if other_word in text.split())\n",
    "                    \n",
    "                    # Extract contexts with collocate_window words to the left and right\n",
    "                    for filename, text in window_texts:\n",
    "                        words = word_tokenize(text)\n",
    "                        for i in range(len(words)):\n",
    "                            if words[i].lower() == word_of_interest.lower():\n",
    "                                start = max(0, i - collocate_window)\n",
    "                                end = min(len(words), i + collocate_window + 1)\n",
    "                                context = words[start:end]\n",
    "                                if other_word.lower() in [w.lower() for w in context]:\n",
    "                                    context_str = ' '.join(context)\n",
    "                                    collocate_contexts[filename].append({\n",
    "                                        'collocate': other_word,\n",
    "                                        'context': context_str\n",
    "                                    })\n",
    "\n",
    "                    collocate_stats.append({\n",
    "                        'word': other_word,\n",
    "                        'total_corpus': word_freq,\n",
    "                        'expected_freq': expected_freq,\n",
    "                        'observed_freq': observed_freq,\n",
    "                        'num_texts': num_texts,\n",
    "                        'pmi': pmi\n",
    "                    })\n",
    "            \n",
    "            collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "            top_collocates = collocate_stats[:top_n]\n",
    "            collocates_per_window[f\"{window_start}-{window_end}\"] = {\n",
    "                'collocates': top_collocates,\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_texts': total_texts,\n",
    "                'contexts': {k: v for k, v in collocate_contexts.items() if any(c['collocate'] in [t['word'] for t in top_collocates] for c in v)}\n",
    "            }\n",
    "            print(f\"Years {window_start}-{window_end}: {len(collocate_stats)} collocates found\")\n",
    "\n",
    "        return collocates_per_window\n",
    "\n",
    "    texts_by_year = read_texts_by_year(text_dirs)\n",
    "    collocates_by_window = get_collocates_by_sliding_window(texts_by_year)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving results to CSV files...\")\n",
    "\n",
    "    for window, data in collocates_by_window.items():\n",
    "        filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                             'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "            \n",
    "            for i, collocate in enumerate(data['collocates'], 1):\n",
    "                writer.writerow([\n",
    "                    i,\n",
    "                    collocate['word'],\n",
    "                    collocate['total_corpus'],\n",
    "                    f\"{collocate['expected_freq']:.2f}\",\n",
    "                    collocate['observed_freq'],\n",
    "                    collocate['num_texts'],\n",
    "                    f\"{collocate['pmi']:.2f}\"\n",
    "                ])\n",
    "            \n",
    "            writer.writerow([])\n",
    "            writer.writerow(['Total tokens in window', data['total_tokens']])\n",
    "            writer.writerow(['Total texts in window', data['total_texts']])\n",
    "\n",
    "        contexts_dir = os.path.join(output_dir, f'contexts_{window}')\n",
    "        os.makedirs(contexts_dir, exist_ok=True)\n",
    "        \n",
    "        for filename, contexts in data['contexts'].items():\n",
    "            context_file = os.path.join(contexts_dir, f'contexts_{os.path.splitext(filename)[0]}.csv')\n",
    "            with open(context_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(['Filename', 'Collocate', 'Context'])\n",
    "                for context in contexts:\n",
    "                    writer.writerow([filename, context['collocate'], context['context']])\n",
    "\n",
    "    print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "    print(f\"Contexts saved to separate CSV files for each text in subdirectories of '{output_dir}'.\")\n",
    "\n",
    "analyze_collocates([r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\", r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstl\", r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rsta\"], \"collocate_results_combined\", \"fact\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
