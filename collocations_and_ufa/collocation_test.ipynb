{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix:\n",
    "- make sure it takes the outlier into account accurately (or skip it!)\n",
    "- make sure the code isnt bullshit (since chatgpt etc)\n",
    "- make sure its 5 right and 5 left, sliding window of years\n",
    "- maybe use a better measure\n",
    "- make it a more concise set of functions\n",
    "- make sure no weird words make it through, such as \"7In\", or find out their context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING CONTEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text processing...\n",
      "Reading text files...\n",
      "Text files read and organized by year.\n",
      "Analyzing collocates by sliding window...\n",
      "Years 1887-1891: 243 collocates found\n",
      "Years 1888-1892: 214 collocates found\n",
      "Years 1889-1893: 237 collocates found\n",
      "Years 1890-1894: 320 collocates found\n",
      "Years 1891-1895: 359 collocates found\n",
      "Years 1892-1896: 337 collocates found\n",
      "Years 1893-1897: 357 collocates found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "text_dir = r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\"\n",
    "\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r'rst[bl]?_(\\d{4})', filename)\n",
    "    if not match:\n",
    "        match = re.search(r'rst[bl]?(\\d{4})', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def read_texts_by_year(directory):\n",
    "    texts_by_year = defaultdict(list)\n",
    "    print(\"Reading text files...\")\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            year = extract_year_from_filename(filename)\n",
    "            if year:\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                texts_by_year[year].append(text)\n",
    "    print(\"Text files read and organized by year.\")\n",
    "    return texts_by_year\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "def get_collocates_by_sliding_window(texts_by_year, word_of_interest, window_size=5):\n",
    "    collocates_per_window = {}\n",
    "    all_years = sorted(texts_by_year.keys())\n",
    "    start_year = min(all_years)\n",
    "    end_year = max(all_years)\n",
    "\n",
    "    print(\"Analyzing collocates by sliding window...\")\n",
    "    for window_start in range(start_year, end_year - window_size + 2):\n",
    "        window_end = window_start + window_size - 1\n",
    "        window_texts = []\n",
    "        for year in range(window_start, window_end + 1):\n",
    "            if year in texts_by_year:\n",
    "                window_texts.extend(texts_by_year[year])\n",
    "        \n",
    "        all_tokens = []\n",
    "        for text in window_texts:\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [token.lower() for token in tokens if token not in string.punctuation and token.lower() not in stop_words and not token.isdigit()] #removing tokens already here (stopwords, punctuation, digits)\n",
    "            all_tokens.extend(tokens) \n",
    "        \n",
    "        tokens = lemmatize_tokens(all_tokens) #should allow for seeing different forms of fact, but potentially bad to lemmatize?\n",
    "        \n",
    "        total_tokens = len(tokens) #total tokens dont include removed ones; is this bad?\n",
    "        total_texts = len(window_texts)\n",
    "        \n",
    "        bigram_measures = BigramAssocMeasures()\n",
    "        finder = BigramCollocationFinder.from_words(tokens)\n",
    "        finder.apply_freq_filter(2) #maybe make it higher to include less (2 is quite low)\n",
    "        \n",
    "        collocations = finder.score_ngrams(bigram_measures.pmi)\n",
    "        \n",
    "        collocate_stats = []\n",
    "        collocate_contexts = []\n",
    "\n",
    "        for bigram, pmi in collocations:\n",
    "            if word_of_interest in bigram:\n",
    "                other_word = bigram[0] if bigram[1] == word_of_interest else bigram[1] #dont fully understand this yet, are bigrams 5L 5R?\n",
    "                observed_freq = finder.ngram_fd[bigram]\n",
    "                word_freq = finder.word_fd[other_word]\n",
    "                expected_freq = (finder.word_fd[word_of_interest] * word_freq) / total_tokens\n",
    "                num_texts = sum(1 for text in window_texts if other_word in text.split())\n",
    "                \n",
    "                # Extract contexts with 5 words to the left and right  (BUT THE BIGRAM SHOULD BE 5L 5R)\n",
    "                for text in window_texts:\n",
    "                    words = text.split()\n",
    "                    for i, word in enumerate(words):\n",
    "                        if word == word_of_interest and other_word in words[max(0, i-5):i+6]:\n",
    "                            context = ' '.join(words[max(0, i-5):i+6])\n",
    "                            collocate_contexts.append(f\"Years {window_start}-{window_end}: {context}\")      \n",
    "\n",
    "                collocate_stats.append({\n",
    "                    'word': other_word,\n",
    "                    'total_corpus': word_freq,\n",
    "                    'expected_freq': expected_freq,\n",
    "                    'observed_freq': observed_freq,\n",
    "                    'num_texts': num_texts,\n",
    "                    'pmi': pmi\n",
    "                })\n",
    "        \n",
    "        collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "        collocates_per_window[f\"{window_start}-{window_end}\"] = {\n",
    "            'collocates': collocate_stats[:20],\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_texts': total_texts,\n",
    "            'contexts': collocate_contexts\n",
    "        }\n",
    "        print(f\"Years {window_start}-{window_end}: {len(collocate_stats)} collocates found\")\n",
    "\n",
    "    return collocates_per_window\n",
    "\n",
    "print(\"Starting text processing...\")\n",
    "texts_by_year = read_texts_by_year(text_dir)\n",
    "collocates_by_window = get_collocates_by_sliding_window(texts_by_year, \"fact\")\n",
    "\n",
    "# Saving collocation results to CSV files\n",
    "output_dir = 'collocate_results_rstb'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(\"Saving results to CSV files...\")\n",
    "\n",
    "for window, data in collocates_by_window.items():\n",
    "    filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                         'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "        \n",
    "        for i, collocate in enumerate(data['collocates'], 1):\n",
    "            writer.writerow([\n",
    "                i,\n",
    "                collocate['word'],\n",
    "                collocate['total_corpus'],\n",
    "                f\"{collocate['expected_freq']:.2f}\",\n",
    "                collocate['observed_freq'],\n",
    "                collocate['num_texts'],\n",
    "                f\"{collocate['pmi']:.2f}\"\n",
    "            ])\n",
    "        \n",
    "        # Add summary statistics at the end of each file\n",
    "        writer.writerow([])\n",
    "        writer.writerow(['Total tokens in window', data['total_tokens']])\n",
    "        writer.writerow(['Total texts in window', data['total_texts']])\n",
    "\n",
    "    # Saving collocate contexts to a text file for each window\n",
    "    context_file = os.path.join(output_dir, f'collocate_contexts_{window}.txt')\n",
    "    with open(context_file, 'w', encoding='utf-8') as f:\n",
    "        for context in data['contexts']:\n",
    "            f.write(context + '\\n')\n",
    "\n",
    "print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "print(f\"Contexts saved to separate files for each window in the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r'rst[bl]?_(\\d{4})', filename)\n",
    "    if not match:\n",
    "        match = re.search(r'rst[bl]?(\\d{4})', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "\n",
    "def analyze_collocates(text_dir, output_dir, word_of_interest, window_size=5):\n",
    "    def read_texts_by_year(directory):\n",
    "        texts_by_year = defaultdict(list)\n",
    "        print(\"Reading text files...\")\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                year = extract_year_from_filename(filename)\n",
    "                if year:\n",
    "                    with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                        text = file.read()\n",
    "                    texts_by_year[year].append((filename, text))\n",
    "        print(\"Text files read and organized by year.\")\n",
    "        return texts_by_year\n",
    "\n",
    "    def get_collocates_by_sliding_window(texts_by_year):\n",
    "        collocates_per_window = {}\n",
    "        all_years = sorted(texts_by_year.keys())\n",
    "        start_year = min(all_years)\n",
    "        end_year = max(all_years)\n",
    "\n",
    "        print(\"Analyzing collocates by sliding window...\")\n",
    "        for window_start in range(start_year, end_year - window_size + 2):\n",
    "            window_end = window_start + window_size - 1\n",
    "            window_texts = []\n",
    "            for year in range(window_start, window_end + 1):\n",
    "                if year in texts_by_year:\n",
    "                    window_texts.extend(texts_by_year[year])\n",
    "            \n",
    "            all_tokens = []\n",
    "            for _, text in window_texts:\n",
    "                tokens = word_tokenize(text)\n",
    "                tokens = [token.lower() for token in tokens if token not in string.punctuation and token.lower() not in stop_words and not token.isdigit()] \n",
    "                all_tokens.extend(tokens)\n",
    "            \n",
    "            tokens = lemmatize_tokens(all_tokens)\n",
    "            \n",
    "            total_tokens = len(tokens)\n",
    "            total_texts = len(window_texts)\n",
    "            \n",
    "            bigram_measures = BigramAssocMeasures()\n",
    "            finder = BigramCollocationFinder.from_words(tokens)\n",
    "            finder.apply_freq_filter(2)\n",
    "            \n",
    "            collocations = finder.score_ngrams(bigram_measures.pmi)\n",
    "            \n",
    "            collocate_stats = []\n",
    "            collocate_contexts = defaultdict(list)\n",
    "\n",
    "            for bigram, pmi in collocations:\n",
    "                if word_of_interest in bigram:\n",
    "                    other_word = bigram[0] if bigram[1] == word_of_interest else bigram[1]\n",
    "                    observed_freq = finder.ngram_fd[bigram]\n",
    "                    word_freq = finder.word_fd[other_word]\n",
    "                    expected_freq = (finder.word_fd[word_of_interest] * word_freq) / total_tokens\n",
    "                    num_texts = sum(1 for _, text in window_texts if other_word in text.split())\n",
    "                    \n",
    "                    # Extract contexts with 5 words to the left and right\n",
    "                    for filename, text in window_texts:\n",
    "                        words = word_tokenize(text)\n",
    "                        for i in range(len(words)):\n",
    "                            if words[i].lower() == word_of_interest.lower():\n",
    "                                start = max(0, i - 5)\n",
    "                                end = min(len(words), i + 6)\n",
    "                                context = words[start:end]\n",
    "                                if other_word.lower() in [w.lower() for w in context]:\n",
    "                                    context_str = ' '.join(context)\n",
    "                                    collocate_contexts[filename].append({\n",
    "                                        'collocate': other_word,\n",
    "                                        'context': context_str\n",
    "                                    })\n",
    "\n",
    "                    collocate_stats.append({\n",
    "                        'word': other_word,\n",
    "                        'total_corpus': word_freq,\n",
    "                        'expected_freq': expected_freq,\n",
    "                        'observed_freq': observed_freq,\n",
    "                        'num_texts': num_texts,\n",
    "                        'pmi': pmi\n",
    "                    })\n",
    "            \n",
    "            collocate_stats.sort(key=lambda x: x['pmi'], reverse=True)\n",
    "            collocates_per_window[f\"{window_start}-{window_end}\"] = {\n",
    "                'collocates': collocate_stats[:20],\n",
    "                'total_tokens': total_tokens,\n",
    "                'total_texts': total_texts,\n",
    "                'contexts': collocate_contexts\n",
    "            }\n",
    "            print(f\"Years {window_start}-{window_end}: {len(collocate_stats)} collocates found\")\n",
    "\n",
    "        return collocates_per_window\n",
    "\n",
    "    texts_by_year = read_texts_by_year(text_dir)\n",
    "    collocates_by_window = get_collocates_by_sliding_window(texts_by_year)\n",
    "\n",
    "    # Saving collocation results to CSV files\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving results to CSV files...\")\n",
    "\n",
    "    for window, data in collocates_by_window.items():\n",
    "        filename = os.path.join(output_dir, f'collocates_{window}.csv')\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['No.', 'Word', 'Total no. in window corpus', 'Expected collocate frequency',\n",
    "                             'Observed collocate frequency', 'In no. of texts', 'Mutual Information value'])\n",
    "            \n",
    "            for i, collocate in enumerate(data['collocates'], 1):\n",
    "                writer.writerow([\n",
    "                    i,\n",
    "                    collocate['word'],\n",
    "                    collocate['total_corpus'],\n",
    "                    f\"{collocate['expected_freq']:.2f}\",\n",
    "                    collocate['observed_freq'],\n",
    "                    collocate['num_texts'],\n",
    "                    f\"{collocate['pmi']:.2f}\"\n",
    "                ])\n",
    "            \n",
    "            # Add summary statistics at the end of each file\n",
    "            writer.writerow([])\n",
    "            writer.writerow(['Total tokens in window', data['total_tokens']])\n",
    "            writer.writerow(['Total texts in window', data['total_texts']])\n",
    "\n",
    "        # Saving collocate contexts to separate CSV files for each text\n",
    "        contexts_dir = os.path.join(output_dir, f'contexts_{window}')\n",
    "        os.makedirs(contexts_dir, exist_ok=True)\n",
    "        \n",
    "        for filename, contexts in data['contexts'].items():\n",
    "            context_file = os.path.join(contexts_dir, f'contexts_{os.path.splitext(filename)[0]}.csv')\n",
    "            with open(context_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(['Filename', 'Collocate', 'Context'])\n",
    "                for context in contexts:\n",
    "                    writer.writerow([filename, context['collocate'], context['context']])\n",
    "\n",
    "    print(f\"Collocate analysis completed. Results saved in the '{output_dir}' directory.\")\n",
    "    print(f\"Contexts saved to separate CSV files for each text in subdirectories of '{output_dir}'.\")\n",
    "\n",
    "analyze_collocates(r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstb\", \"collocate_results_rstb_latest\", \"fact\")\n",
    "analyze_collocates(r\"D:\\Fact_fiction_corpus\\texts\\royal society\\txt_rstl\", \"collocate_results_rstl_latest\", \"fact\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
